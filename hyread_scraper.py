#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¡ƒåœ’å¸‚ç«‹åœ–æ›¸é¤¨ HyRead é›»å­æ›¸è‡ªå‹•å€Ÿé–±å·¥å…·
ä½¿ç”¨ Playwright å’Œ Google Gemini API é€²è¡Œé©—è­‰ç¢¼è¾¨è­˜å’Œè‡ªå‹•å€Ÿé–±
"""

import asyncio
import os
import re
import sys
from pathlib import Path
from datetime import datetime
from typing import List, Dict
from urllib.parse import urljoin
import hashlib

from dotenv import load_dotenv
from playwright.async_api import async_playwright, Page, Browser, FrameLocator
import httpx
from loguru import logger

try:
    import google.generativeai as genai
    HAS_GEMINI = True
except ImportError:
    HAS_GEMINI = False

# é…ç½® loguru
logger.remove()  # ç§»é™¤é»˜èª handler
logger.add(
    sys.stderr,
    format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <level>{message}</level>",
    level="INFO",
    colorize=True
)
logger.add(
    "logs/hyread_scraper_{time:YYYY-MM-DD}.log",
    format="{time:YYYY-MM-DD HH:mm:ss} | {level: <8} | {message}",
    level="DEBUG",
    rotation="00:00",
    retention="7 days",
    compression="zip"
)


class HyReadScraper:
    """æ¡ƒåœ’å¸‚ç«‹åœ–æ›¸é¤¨ HyRead é›»å­æ›¸è‡ªå‹•å€Ÿé–±é¡åˆ¥"""

    def __init__(self, env_file: str = ".env_hyread"):
        """
        åˆå§‹åŒ–å€Ÿé–±å™¨

        Args:
            env_file: ç’°å¢ƒè®Šæ•¸æª”æ¡ˆè·¯å¾‘
        """
        # è¼‰å…¥ç’°å¢ƒè®Šæ•¸
        env_path = Path(env_file)
        if not env_path.exists():
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°ç’°å¢ƒè®Šæ•¸æª”æ¡ˆ: {env_file}")

        load_dotenv(env_path)

        # è®€å–è¨­å®š
        self.account = os.getenv("HYREAD_ACCOUNT")
        self.password = os.getenv("HYREAD_PASSWORD")
        self.google_api_key = os.getenv("OPENAI_API_KEY")
        self.model_name = os.getenv("OPENAI_MODEL", "gemini-2.0-flash-exp")
        self.book_id = os.getenv("HYREAD_BOOK_ID", "279235")  # é è¨­æ›¸ç± ID
        self.captcha_mode = os.getenv("CAPTCHA_MODE", "manual").lower()  # é©—è­‰ç¢¼æ¨¡å¼
        self.enable_scraping = os.getenv("ENABLE_SCRAPING", "true").lower() == "true"  # æ˜¯å¦å•Ÿç”¨çˆ¬èŸ²
        self.max_pages = int(os.getenv("MAX_PAGES", "999"))  # æœ€å¤§çˆ¬å–é æ•¸
        self.download_images = os.getenv("DOWNLOAD_IMAGES", "true").lower() == "true"  # æ˜¯å¦ä¸‹è¼‰åœ–ç‰‡
        self.image_only_mode = os.getenv("IMAGE_ONLY_MODE", "false").lower() == "true"  # ç´”åœ–ç‰‡æ›¸ç±æ¨¡å¼
        
        # ç¿»é ç­–ç•¥ç›¸é—œ
        self.smart_page_turn = os.getenv("SMART_PAGE_TURN", "true").lower() == "true"  # æ˜¯å¦å•Ÿç”¨æ™ºèƒ½ç¿»é 
        self.pages_per_turn = int(os.getenv("PAGES_PER_TURN", "3"))  # å›ºå®šç¿»é æ•¸é‡ï¼ˆç•¶æ™ºèƒ½ç¿»é é—œé–‰æ™‚ï¼‰
        
        # ç¿»é æŒ‰éµè¨­å®š
        page_turn_key = os.getenv("PAGE_TURN_KEY", "ArrowRight")
        # é©—è­‰æŒ‰éµå€¼æ˜¯å¦æœ‰æ•ˆ
        valid_keys = ["ArrowRight", "ArrowLeft", "ArrowUp", "ArrowDown"]
        if page_turn_key not in valid_keys:
            logger.warning(f"âš ï¸  ç„¡æ•ˆçš„ç¿»é æŒ‰éµ: {page_turn_key}ï¼Œä½¿ç”¨é è¨­å€¼ ArrowRight")
            page_turn_key = "ArrowRight"
        self.page_turn_key = page_turn_key

        # åœ–ç‰‡ä¸‹è¼‰ç›¸é—œ
        self.images_dir = None
        self.downloaded_images = {}  # URL -> æœ¬åœ°è·¯å¾‘æ˜ å°„
        self.canvas_hashes = set()  # ç”¨æ–¼ Canvas å»é‡çš„ MD5 hash é›†åˆ
        self.book_title = None  # æ›¸å

        # é©—è­‰å¿…è¦åƒæ•¸
        if not all([self.account, self.password]):
            raise ValueError("è«‹ç¢ºä¿ .env_hyread ä¸­åŒ…å« HYREAD_ACCOUNT å’Œ HYREAD_PASSWORD")

        # å¦‚æœä½¿ç”¨è‡ªå‹•æ¨¡å¼ï¼Œéœ€è¦æª¢æŸ¥ API Key å’Œ Gemini SDK
        if self.captcha_mode == "auto":
            if not self.google_api_key:
                raise ValueError("è‡ªå‹•æ¨¡å¼éœ€è¦ OPENAI_API_KEYï¼Œæˆ–å°‡ CAPTCHA_MODE è¨­ç‚º manual")

            if not HAS_GEMINI:
                raise ImportError(
                    "è«‹å®‰è£ Google Gemini SDK:\n"
                    "pip install google-generativeai Pillow"
                )

            # è¨­å®š Gemini API
            genai.configure(api_key=self.google_api_key)

            # åˆå§‹åŒ–æ¨¡å‹
            self.model = genai.GenerativeModel(self.model_name)

        # URL è¨­å®š
        self.login_url = "https://tycccgov.ebook.hyread.com.tw/Template/RWD3.0/liblogin.jsp"
        self.base_url = "https://tycccgov.ebook.hyread.com.tw"

        logger.success(f"âœ… å·²è¼‰å…¥è¨­å®š:")
        logger.info(f"   - å¸³è™Ÿ: {self.account}")
        logger.info(f"   - é©—è­‰ç¢¼æ¨¡å¼: {'è‡ªå‹•è¾¨è­˜ (Gemini)' if self.captcha_mode == 'auto' else 'æ‰‹å‹•è¼¸å…¥'}")
        if self.captcha_mode == "auto":
            logger.info(f"   - Gemini æ¨¡å‹: {self.model_name}")
        logger.info(f"   - ç›®æ¨™æ›¸ç± ID: {self.book_id}")
        logger.info(f"   - çˆ¬èŸ²æ¨¡å¼: {'å•Ÿç”¨' if self.enable_scraping else 'åœç”¨'}")
        if self.enable_scraping:
            logger.info(f"   - æœ€å¤§çˆ¬å–é æ•¸: {self.max_pages}")
            logger.info(f"   - ä¸‹è¼‰åœ–ç‰‡: {'æ˜¯' if self.download_images else 'å¦'}")
            logger.info(f"   - ç´”åœ–ç‰‡æ›¸ç±æ¨¡å¼: {'æ˜¯ (Canvas Only)' if self.image_only_mode else 'å¦ (HTML + Canvas)'}")
            logger.info(f"   - ç¿»é ç­–ç•¥: {'æ™ºèƒ½ç¿»é ' if self.smart_page_turn else f'å›ºå®šç¿»é ï¼ˆæ¯æ¬¡ {self.pages_per_turn} é ï¼‰'}")
            
            # é¡¯ç¤ºç¿»é æŒ‰éµï¼ˆåŠ ä¸Šå‹å–„çš„ä¸­æ–‡èªªæ˜ï¼‰
            key_names = {
                "ArrowRight": "å³éµ (â†’)",
                "ArrowLeft": "å·¦éµ (â†)",
                "ArrowUp": "ä¸Šéµ (â†‘)",
                "ArrowDown": "ä¸‹éµ (â†“)"
            }
            key_display = key_names.get(self.page_turn_key, self.page_turn_key)
            logger.info(f"   - ç¿»é æŒ‰éµ: {key_display}")

    async def solve_captcha(self, page: Page) -> str:
        """
        è§£æ±ºé©—è­‰ç¢¼

        Args:
            page: Playwright é é¢ç‰©ä»¶

        Returns:
            è¾¨è­˜å‡ºçš„é©—è­‰ç¢¼æ–‡å­—
        """
        # å®šä½é©—è­‰ç¢¼åœ–ç‰‡
        captcha_img = page.locator("#conImg")
        await captcha_img.wait_for(state="visible", timeout=10000)

        if self.captcha_mode == "manual":
            # æ‰‹å‹•æ¨¡å¼ï¼šé¡¯ç¤ºé©—è­‰ç¢¼ä¸¦ç­‰å¾…ä½¿ç”¨è€…è¼¸å…¥
            logger.info("ğŸ“¸ é©—è­‰ç¢¼åœ–ç‰‡å·²é¡¯ç¤ºåœ¨ç€è¦½å™¨ä¸­")
            logger.info("ğŸ‘€ è«‹æŸ¥çœ‹ç€è¦½å™¨è¦–çª—ä¸­çš„é©—è­‰ç¢¼")
            logger.info("="*60)

            # ç­‰å¾…ä¸€ä¸‹è®“ä½¿ç”¨è€…çœ‹æ¸…æ¥šé©—è­‰ç¢¼
            await asyncio.sleep(1)

            # å¾å‘½ä»¤åˆ—è®€å–ä½¿ç”¨è€…è¼¸å…¥
            captcha_text = input("âŒ¨ï¸  è«‹è¼¸å…¥é©—è­‰ç¢¼: ").strip()

            if not captcha_text:
                raise ValueError("é©—è­‰ç¢¼ä¸èƒ½ç‚ºç©º")

            logger.success(f"âœ… æ‚¨è¼¸å…¥çš„é©—è­‰ç¢¼: {captcha_text}")
            return captcha_text

        else:
            # è‡ªå‹•æ¨¡å¼ï¼šä½¿ç”¨ Gemini API è¾¨è­˜
            logger.info("ğŸ“¸ æ­£åœ¨æˆªå–é©—è­‰ç¢¼åœ–ç‰‡...")

            # æˆªå–é©—è­‰ç¢¼åœ–ç‰‡
            captcha_screenshot = await captcha_img.screenshot()

            logger.info("ğŸ¤– æ­£åœ¨å‘¼å« Google Gemini API è¾¨è­˜é©—è­‰ç¢¼...")

            try:
                # æº–å‚™åœ–ç‰‡
                import PIL.Image
                import io
                image = PIL.Image.open(io.BytesIO(captcha_screenshot))

                # å‘¼å« Gemini Vision API
                prompt = (
                    "Please identify the text or numbers in this CAPTCHA image. "
                    "Return ONLY the CAPTCHA text without any explanation, punctuation, or formatting. "
                    "If you see letters and numbers, return them exactly as shown."
                )

                response = self.model.generate_content([prompt, image])

                # æª¢æŸ¥å›æ‡‰
                if not response.text:
                    raise ValueError("Gemini API å›æ‡‰å…§å®¹ç‚ºç©º")

                captcha_text = response.text.strip()
                logger.success(f"âœ… é©—è­‰ç¢¼è¾¨è­˜çµæœ: {captcha_text}")
                return captcha_text

            except Exception as e:
                logger.error(f"âŒ Gemini API å‘¼å«å¤±æ•—: {e}")
                raise

    async def login(self, page: Page) -> bool:
        """
        åŸ·è¡Œè‡ªå‹•ç™»å…¥

        Args:
            page: Playwright é é¢ç‰©ä»¶

        Returns:
            ç™»å…¥æ˜¯å¦æˆåŠŸ
        """
        logger.info("\n" + "="*60)
        logger.info("ğŸš€ é–‹å§‹è‡ªå‹•ç™»å…¥æµç¨‹")
        logger.info("="*60)

        # å‰å¾€ç™»å…¥é é¢
        logger.info(f"ğŸ“„ æ­£åœ¨å‰å¾€ç™»å…¥é é¢: {self.login_url}")
        await page.goto(self.login_url)
        await asyncio.sleep(2)

        # å¡«å¯«å¸³è™Ÿ
        logger.info(f"âœï¸  å¡«å¯«å¸³è™Ÿ: {self.account}")
        account_input = page.locator('input[name="account2"]')
        await account_input.wait_for(state="visible", timeout=10000)
        await account_input.fill(self.account)
        await asyncio.sleep(0.5)

        # å¡«å¯«å¯†ç¢¼
        logger.info("ğŸ”’ å¡«å¯«å¯†ç¢¼...")
        password_input = page.locator('input[name="passwd2"]')
        await password_input.fill(self.password)
        await asyncio.sleep(0.5)

        # è¾¨è­˜ä¸¦å¡«å¯«é©—è­‰ç¢¼
        max_retries = 3
        for attempt in range(1, max_retries + 1):
            logger.info(f"\nğŸ” é©—è­‰ç¢¼è¾¨è­˜å˜—è©¦ {attempt}/{max_retries}")

            try:
                captcha_text = await self.solve_captcha(page)

                # å¡«å¯«é©—è­‰ç¢¼
                logger.info(f"âœï¸  å¡«å¯«é©—è­‰ç¢¼: {captcha_text}")
                valicode_input = page.locator('input[name="valicode"]')
                await valicode_input.fill("")  # å…ˆæ¸…ç©º
                await valicode_input.fill(captcha_text)
                await asyncio.sleep(0.5)

                # é»æ“Šç™»å…¥æŒ‰éˆ•
                logger.info("ğŸ–±ï¸  é»æ“Šç™»å…¥æŒ‰éˆ•...")
                login_button = page.locator('a[href="javascript:docheck();"] .login-btn')
                await login_button.click()

                # ç­‰å¾…é é¢å°èˆª
                await asyncio.sleep(3)

                # æª¢æŸ¥æ˜¯å¦ç™»å…¥æˆåŠŸ
                current_url = page.url
                logger.info(f"ğŸ“ ç•¶å‰ URL: {current_url}")

                if "ebook.hyread.com.tw" in current_url and "index.jsp" in current_url:
                    logger.info("\n" + "="*60)
                    logger.success("âœ… ç™»å…¥æˆåŠŸï¼")
                    logger.info("="*60)
                    return True

                elif current_url == self.login_url:
                    logger.warning(f"âš ï¸  é©—è­‰ç¢¼å¯èƒ½éŒ¯èª¤ï¼Œæº–å‚™é‡è©¦...")

                    if attempt < max_retries:
                        await valicode_input.fill("")
                        await asyncio.sleep(1)
                        continue
                    else:
                        logger.info(f"\nâŒ å·²é”åˆ°æœ€å¤§é‡è©¦æ¬¡æ•¸ ({max_retries})ï¼Œç™»å…¥å¤±æ•—")
                        return False

            except Exception as e:
                logger.error(f"âŒ é©—è­‰ç¢¼è¾¨è­˜å¤±æ•—: {e}")
                if attempt < max_retries:
                    logger.info("â³ ç­‰å¾…å¾Œé‡è©¦...")
                    await asyncio.sleep(2)
                    continue
                else:
                    raise

        return False

    async def check_and_borrow_book(self, page: Page, book_id: str) -> bool:
        """
        æª¢æŸ¥ä¸¦å€Ÿé–±æ›¸ç±

        Args:
            page: Playwright é é¢ç‰©ä»¶
            book_id: æ›¸ç± ID

        Returns:
            å€Ÿé–±æ˜¯å¦æˆåŠŸ
        """
        logger.info("\n" + "="*60)
        logger.info("ğŸ“š é–‹å§‹æª¢æŸ¥æ›¸ç±")
        logger.info("="*60)

        # å‰å¾€æ›¸ç±è©³æƒ…é é¢
        book_url = f"{self.base_url}/bookDetail.jsp?id={book_id}"
        logger.info(f"ğŸ“„ æ­£åœ¨å‰å¾€æ›¸ç±é é¢: {book_url}")
        await page.goto(book_url)
        await asyncio.sleep(2)

        # æå–æ›¸åï¼ˆå–åˆ°ç¬¬ä¸€å€‹æ¨™é»ç¬¦è™Ÿï¼‰
        try:
            book_title_element = page.locator('.book-detail h3')
            if await book_title_element.count() > 0:
                full_title = await book_title_element.text_content()

                if full_title:
                    # å–åˆ°ç¬¬ä¸€å€‹æ¨™é»ç¬¦è™Ÿï¼ˆï¼š:ã€ã€‚ï¼ï¼Ÿï¼‰
                    import re
                    match = re.search(r'^([^ï¼š:ã€ã€‚ï¼ï¼Ÿ]+)', full_title.strip())
                    if match:
                        short_title = match.group(1).strip()
                        self.book_title = short_title
                        logger.info(f"ğŸ“– æ›¸å: {short_title}")
                    else:
                        self.book_title = full_title.strip()
                        logger.info(f"ğŸ“– æ›¸å: {self.book_title}")
        except Exception as e:
            logger.warning(f"âš ï¸  ç„¡æ³•æå–æ›¸å: {e}")
            self.book_title = f"book_{book_id}"

        # æª¢æŸ¥ç·šä¸Šé–±è®€æŒ‰éˆ•
        try:
            # æ–¹æ¡ˆ1: å®šä½ç·šä¸Šé–±è®€æŒ‰éˆ•ï¼ˆæœªå€Ÿé–±çš„æƒ…æ³ï¼‰
            read_button = page.locator('button.btn-collect:has-text("ç·šä¸Šé–±è®€")')
            button_to_click = None
            is_already_borrowed = False

            # æª¢æŸ¥æŒ‰éˆ•æ˜¯å¦å­˜åœ¨
            if await read_button.count() > 0:
                # ç²å–æŒ‰éˆ•çš„ title å±¬æ€§
                button_title = await read_button.get_attribute('title')
                logger.info(f"ğŸ“Š æŒ‰éˆ•ç‹€æ…‹: {button_title}")

                # ä½¿ç”¨æ­£å‰‡è¡¨é”å¼æå–å¯ç”¨æ•¸é‡
                match = re.search(r'ç·šä¸Šé–±è®€äººæ•¸.*?å°šæœ‰(\d+)æœ¬', button_title, re.DOTALL)

                if match:
                    available_count = int(match.group(1))
                    logger.info(f"ğŸ“Š å¯å€Ÿé–±æ•¸é‡: {available_count} æœ¬")

                    if available_count > 0:
                        logger.success("âœ… æ›¸ç±å¯å€Ÿé–±ï¼Œæº–å‚™é»æ“Šç·šä¸Šé–±è®€æŒ‰éˆ•...")
                        button_to_click = read_button
                    else:
                        logger.warning("âš ï¸  ç›®å‰æ²’æœ‰å¯å€Ÿé–±çš„å‰¯æœ¬")
                        return False
                else:
                    logger.warning("âš ï¸  ç„¡æ³•è§£æå¯å€Ÿé–±æ•¸é‡ï¼Œå˜—è©¦ç›´æ¥é»æ“Š...")
                    button_to_click = read_button
            else:
                # æ–¹æ¡ˆ2: æª¢æŸ¥æ˜¯å¦å·²å€Ÿé–±ï¼ˆ"é–‹å•Ÿ"æŒ‰éˆ•ï¼‰
                logger.info("ğŸ“– æœªæ‰¾åˆ°ã€Œç·šä¸Šé–±è®€ã€æŒ‰éˆ•ï¼Œæª¢æŸ¥æ˜¯å¦å·²å€Ÿé–±...")
                open_button = page.locator('input[value="é–‹å•Ÿ"]')
                
                if await open_button.count() > 0:
                    logger.success("âœ… æ›¸ç±å·²å€Ÿé–±ï¼Œæ‰¾åˆ°ã€Œé–‹å•Ÿã€æŒ‰éˆ•")
                    button_to_click = open_button
                    is_already_borrowed = True
                else:
                    logger.error("âŒ æ‰¾ä¸åˆ°ã€Œç·šä¸Šé–±è®€ã€æˆ–ã€Œé–‹å•Ÿã€æŒ‰éˆ•")
                    return False

            # é»æ“ŠæŒ‰éˆ•ï¼ˆç·šä¸Šé–±è®€ æˆ– é–‹å•Ÿï¼‰
            if button_to_click:
                if is_already_borrowed:
                    logger.info("ğŸ–±ï¸  é»æ“Šã€Œé–‹å•Ÿã€æŒ‰éˆ•...")
                else:
                    logger.info("ğŸ–±ï¸  é»æ“Šã€Œç·šä¸Šé–±è®€ã€æŒ‰éˆ•...")
                
                await button_to_click.click()
                await asyncio.sleep(3)

                # æª¢æŸ¥æ˜¯å¦æˆåŠŸé–‹å•Ÿé–±è®€é é¢
                # å¯èƒ½æœƒé–‹å•Ÿæ–°åˆ†é æˆ–å½ˆå‡ºè¦–çª—
                current_url = page.url
                logger.info(f"ğŸ“ ç•¶å‰ URL: {current_url}")

                # æª¢æŸ¥æ‰€æœ‰é é¢
                all_pages = page.context.pages
                logger.info(f"ğŸ“„ ç›®å‰é–‹å•Ÿçš„é é¢æ•¸: {len(all_pages)}")

                reading_page = None

                if len(all_pages) > 1:
                    logger.success("âœ… å·²é–‹å•Ÿæ–°çš„é–±è®€è¦–çª—")
                    # åˆ‡æ›åˆ°æ–°é é¢
                    reading_page = all_pages[-1]
                    await asyncio.sleep(2)
                    logger.info(f"ğŸ“ é–±è®€é é¢ URL: {reading_page.url}")
                else:
                    # å¦‚æœæ²’æœ‰é–‹å•Ÿæ–°é é¢ï¼Œå¯èƒ½åœ¨ç•¶å‰é é¢ä¸­æ‰“é–‹
                    logger.warning("âš ï¸  æœªåµæ¸¬åˆ°æ–°è¦–çª—ï¼Œæª¢æŸ¥ç•¶å‰é é¢...")

                    # ç­‰å¾…é é¢å¯èƒ½çš„è®ŠåŒ–
                    await asyncio.sleep(2)

                    # æª¢æŸ¥ç•¶å‰é é¢ URL æ˜¯å¦æ”¹è®Š
                    if page.url != current_url or "reader" in page.url.lower():
                        logger.success("âœ… é–±è®€å™¨åœ¨ç•¶å‰é é¢ä¸­æ‰“é–‹")
                        reading_page = page
                    else:
                        # å†ç­‰å¾…ä¸¦é‡æ–°æª¢æŸ¥
                        await asyncio.sleep(3)
                        all_pages = page.context.pages
                        if len(all_pages) > 1:
                            reading_page = all_pages[-1]
                            logger.success(f"âœ… å»¶é²åµæ¸¬åˆ°æ–°è¦–çª—: {reading_page.url}")
                        else:
                            logger.warning("âš ï¸  ä»æœªåµæ¸¬åˆ°é–±è®€è¦–çª—ï¼Œä½¿ç”¨ç•¶å‰é é¢")
                            reading_page = page

                logger.info("\n" + "="*60)
                logger.success("âœ… é–‹å•ŸæˆåŠŸï¼")
                logger.info("="*60)

                # å¦‚æœå•Ÿç”¨çˆ¬èŸ²ï¼Œè¿”å›é–±è®€é é¢ç”¨æ–¼å¾ŒçºŒçˆ¬å–
                if self.enable_scraping:
                    if reading_page:
                        logger.info(f"ğŸ“– å°‡ä½¿ç”¨é é¢é€²è¡Œçˆ¬å–: {reading_page.url}")
                        return reading_page
                    else:
                        logger.error("âŒ ç„¡æ³•ç²å–é–±è®€é é¢")
                        return False
                else:
                    return True

        except Exception as e:
            logger.error(f"âŒ æª¢æŸ¥æˆ–å€Ÿé–±æ›¸ç±æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            import traceback
            traceback.print_exc()
            return False

    async def click_accept_button(self, page: Page) -> bool:
        """
        é»æ“Šã€Œæˆ‘çŸ¥é“äº†ã€æŒ‰éˆ•

        Args:
            page: Playwright é é¢ç‰©ä»¶

        Returns:
            æ˜¯å¦æˆåŠŸé»æ“Š
        """
        try:
            logger.info("\nğŸ” å°‹æ‰¾ã€Œæˆ‘çŸ¥é“äº†ã€æŒ‰éˆ•...")

            # ç­‰å¾…æŒ‰éˆ•å‡ºç¾
            accept_button = page.locator('button:has-text("æˆ‘çŸ¥é“äº†")')

            # ç­‰å¾…æœ€å¤š 10 ç§’
            await accept_button.wait_for(state="visible", timeout=10000)

            logger.info("ğŸ–±ï¸  é»æ“Šã€Œæˆ‘çŸ¥é“äº†ã€æŒ‰éˆ•...")
            await accept_button.click()
            await asyncio.sleep(2)

            logger.success("âœ… å·²é»æ“Šã€Œæˆ‘çŸ¥é“äº†ã€æŒ‰éˆ•")
            return True

        except Exception as e:
            logger.warning(f"âš ï¸  æœªæ‰¾åˆ°æˆ–ç„¡æ³•é»æ“Šã€Œæˆ‘çŸ¥é“äº†ã€æŒ‰éˆ•: {e}")
            return False

    async def handle_reading_progress_popup(self, page: Page) -> bool:
        """
        è™•ç†é–±è®€é€²åº¦å½ˆçª—ï¼ˆå¦‚æœå­˜åœ¨ï¼‰

        Args:
            page: Playwright é é¢ç‰©ä»¶

        Returns:
            æ˜¯å¦è™•ç†äº†å½ˆçª—
        """
        try:
            logger.info("\nğŸ” æª¢æŸ¥æ˜¯å¦æœ‰é–±è®€é€²åº¦å½ˆçª—...")

            # æ›´ç²¾ç¢ºçš„é¸æ“‡å™¨ï¼šåŒæ™‚æª¢æŸ¥ class å’Œæ–‡å­—å…§å®¹
            progress_popup = page.locator('div.reader-popover[aria-label*="é–±è®€é€²åº¦"]')
            
            # å¦‚æœæ²’æ‰¾åˆ°ï¼Œå˜—è©¦ç¬¬äºŒç¨®æ–¹å¼
            if await progress_popup.count() == 0:
                progress_popup = page.locator('div[class*="reader-popover"]:has-text("è«‹å•æ˜¯å¦å‰å¾€")')

            # ç­‰å¾…æœ€å¤š 2 ç§’ï¼Œçµ¦å½ˆçª—è¶³å¤ æ™‚é–“å‡ºç¾
            try:
                await progress_popup.wait_for(state="visible", timeout=2000)
                
                # ç¢ºèªå½ˆçª—çœŸçš„å¯è¦‹
                if not await progress_popup.is_visible():
                    logger.info("â„¹ï¸  æ²’æœ‰é–±è®€é€²åº¦å½ˆçª—ï¼Œç¹¼çºŒåŸ·è¡Œ")
                    return False
                
                # æ‰¾åˆ°äº†å½ˆçª—ï¼Œæå–é€²åº¦ä¿¡æ¯
                popup_text = await progress_popup.text_content()
                logger.info(f"ğŸ“ ç™¼ç¾é–±è®€é€²åº¦å½ˆçª—: {popup_text[:60].replace(chr(10), ' ')}...")
                
                # åœ¨å½ˆçª—å…§éƒ¨æŸ¥æ‰¾ã€Œç•¥éã€æŒ‰éˆ•ï¼ˆæ›´ç²¾ç¢ºï¼‰
                skip_button = progress_popup.locator('button:has-text("ç•¥é")').first
                
                # ç¢ºä¿æŒ‰éˆ•å­˜åœ¨ä¸”å¯é»æ“Š
                if await skip_button.count() > 0:
                    # ç­‰å¾…æŒ‰éˆ•å¯é»æ“Š
                    await skip_button.wait_for(state="visible", timeout=1000)
                    
                    logger.info("ğŸ–±ï¸  é»æ“Šã€Œç•¥éã€æŒ‰éˆ•...")
                    await skip_button.click()
                    
                    # ç­‰å¾…å½ˆçª—æ¶ˆå¤±ï¼ˆé‡è¦ï¼ï¼‰
                    try:
                        await progress_popup.wait_for(state="hidden", timeout=3000)
                        logger.success("âœ… å·²ç•¥éé–±è®€é€²åº¦æç¤ºï¼Œå½ˆçª—å·²é—œé–‰")
                    except:
                        logger.warning("âš ï¸  å½ˆçª—å¯èƒ½æœªå®Œå…¨é—œé–‰ï¼Œç¹¼çºŒåŸ·è¡Œ")
                    
                    # é¡å¤–ç­‰å¾…ï¼Œç¢ºä¿é é¢ç©©å®š
                    await asyncio.sleep(1.5)
                    return True
                else:
                    logger.warning("âš ï¸  æ‰¾ä¸åˆ°ã€Œç•¥éã€æŒ‰éˆ•")
                    return False
                    
            except Exception as timeout_err:
                # æ²’æœ‰å½ˆçª—æˆ–è¶…æ™‚ï¼Œé€™æ˜¯æ­£å¸¸æƒ…æ³
                logger.info("â„¹ï¸  æ²’æœ‰é–±è®€é€²åº¦å½ˆçª—ï¼Œç¹¼çºŒåŸ·è¡Œ")
                return False

        except Exception as e:
            logger.debug(f"æª¢æŸ¥é–±è®€é€²åº¦å½ˆçª—æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return False

    async def get_all_visible_iframes(self, page: Page) -> list:
        """
        ç²å–æ‰€æœ‰å¯è¦‹çš„ iframe

        Args:
            page: Playwright é é¢ç‰©ä»¶

        Returns:
            æ‰€æœ‰å¯è¦‹ iframe çš„ FrameLocator åˆ—è¡¨
        """
        try:
            visible_iframes = []

            # ç›´æ¥æ‰¾åˆ°æ‰€æœ‰ iframe å…ƒç´ 
            iframes = page.locator('iframe')
            iframe_count = await iframes.count()

            logger.info(f"   ğŸ” æ‰¾åˆ° {iframe_count} å€‹ iframe")

            # éæ­·æ‰€æœ‰ iframe
            for i in range(iframe_count):
                iframe_element = iframes.nth(i)

                # æª¢æŸ¥ iframe æ˜¯å¦å¯è¦‹
                is_visible = await iframe_element.is_visible()

                if is_visible:
                    frame_locator = page.frame_locator('iframe').nth(i)
                    visible_iframes.append(frame_locator)
                    logger.info(f"      âœ“ iframe[{i}] å¯è¦‹")
                else:
                    logger.info(f"      âœ— iframe[{i}] ä¸å¯è¦‹")

            if not visible_iframes:
                logger.info("   âš ï¸  æ²’æœ‰æ‰¾åˆ°å¯è¦‹çš„ iframeï¼Œä½¿ç”¨ç¬¬ä¸€å€‹")
                visible_iframes.append(page.frame_locator('iframe').first)

            return visible_iframes

        except Exception as e:
            logger.info(f"   âš ï¸  ç²å– iframe æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            # é™ç´šæ–¹æ¡ˆï¼šè¿”å›ç¬¬ä¸€å€‹ iframe
            return [page.frame_locator('iframe').first]

    async def get_current_iframe(self, page: Page) -> FrameLocator:
        """
        ç²å–ç•¶å‰é¡¯ç¤ºçš„ iframeï¼ˆå‘å¾Œå…¼å®¹çš„æ–¹æ³•ï¼‰

        Args:
            page: Playwright é é¢ç‰©ä»¶

        Returns:
            ç•¶å‰çš„ iframe locator
        """
        visible_iframes = await self.get_all_visible_iframes(page)
        return visible_iframes[0] if visible_iframes else page.frame_locator('iframe').first

    async def extract_html_with_formatting(self, element) -> str:
        """
        æå–å…ƒç´ çš„ HTML ä¸¦ä¿ç•™æ ¼å¼æ¨™ç±¤

        Args:
            element: Playwright å…ƒç´ 

        Returns:
            åŒ…å«æ ¼å¼çš„æ–‡å­—
        """
        try:
            # ç²å–å…ƒç´ çš„ innerHTML
            html = await element.inner_html()

            # è½‰æ› HTML æ ¼å¼ç‚º Markdown æ ¼å¼
            # ç²—é«”ï¼š<strong>, <b> -> **text**
            html = re.sub(r'<strong>(.*?)</strong>', r'**\1**', html)
            html = re.sub(r'<b>(.*?)</b>', r'**\1**', html)

            # æ–œé«”ï¼š<em>, <i> -> *text*
            html = re.sub(r'<em>(.*?)</em>', r'*\1*', html)
            html = re.sub(r'<i>(.*?)</i>', r'*\1*', html)

            # ç‰¹æ®Š span é¡ï¼šgfontorange -> ç²—é«”
            html = re.sub(r'<span[^>]*class="[^"]*gfontorange[^"]*"[^>]*>(.*?)</span>', r'**\1**', html)
            
            # Footnote å¼•ç”¨ï¼š<a class="ref" ...>1</a> -> [^1]
            # æå– footnote ç·¨è™Ÿä¸¦è½‰æ›ç‚º Markdown å¼•ç”¨æ ¼å¼
            html = re.sub(r'<a[^>]*class="[^"]*ref[^"]*"[^>]*>(\d+)</a>', r'[^\1]', html)
            
            # ç§»é™¤å…¶ä»– HTML æ¨™ç±¤ä½†ä¿ç•™å…§å®¹
            html = re.sub(r'<span[^>]*>(.*?)</span>', r'\1', html)
            html = re.sub(r'<div[^>]*>(.*?)</div>', r'\1', html)
            html = re.sub(r'<br\s*/?>', '\n', html)

            # ç§»é™¤æ‰€æœ‰å‰©é¤˜çš„ HTML æ¨™ç±¤
            html = re.sub(r'<[^>]+>', '', html)

            return html.strip()

        except Exception as e:
            # å¦‚æœå‡ºéŒ¯ï¼Œè¿”å›ç´”æ–‡å­—
            return await element.text_content()

    async def get_base_url_from_iframe(self, page: Page) -> str:
        """
        å¾ iframe ç²å– base URL

        Args:
            page: Playwright é é¢ç‰©ä»¶

        Returns:
            base URL æˆ–ç©ºå­—ä¸²
        """
        try:
            iframe = await self.get_current_iframe(page)
            base_element = iframe.locator('base').first
            base_href = await base_element.get_attribute('href')
            return base_href or ''
        except:
            return ''

    async def scrape_page_content(self, page: Page) -> Dict[str, any]:
        """
        æŠ“å–ç•¶å‰é é¢çš„å…§å®¹ï¼ˆå¾æ‰€æœ‰å¯è¦‹çš„ iframeï¼‰

        Args:
            page: Playwright é é¢ç‰©ä»¶

        Returns:
            åŒ…å«æ¨™é¡Œã€æ®µè½å’Œåœ–ç‰‡çš„å­—å…¸
        """
        try:
            # ç²å–æ‰€æœ‰å¯è¦‹çš„ iframe
            visible_iframes = await self.get_all_visible_iframes(page)

            content = {
                'headings': [],
                'paragraphs': [],
                'images': []
            }

            # å¾æ‰€æœ‰å¯è¦‹çš„ iframe ä¸­æŠ“å–å…§å®¹
            for iframe_index, iframe in enumerate(visible_iframes):
                logger.info(f"      ğŸ“„ æ­£åœ¨æŠ“å– iframe[{iframe_index}] çš„å…§å®¹...")
                iframe_content = await self._scrape_from_single_iframe(iframe)

                # åˆä½µå…§å®¹
                content['headings'].extend(iframe_content['headings'])
                content['paragraphs'].extend(iframe_content['paragraphs'])
                content['images'].extend(iframe_content['images'])

                logger.info(f"         æ‰¾åˆ°: æ¨™é¡Œ={len(iframe_content['headings'])}, æ®µè½={len(iframe_content['paragraphs'])}, åœ–ç‰‡={len(iframe_content['images'])}")

            return content

        except Exception as e:
            logger.warning(f"âš ï¸  æŠ“å–é é¢å…§å®¹æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return {'headings': [], 'paragraphs': [], 'images': []}

    async def _extract_figure_content(self, figure_element) -> dict:
        """
        å¾ figure å…ƒç´ ä¸­æå–åœ–ç‰‡å’Œèªªæ˜æ–‡å­—

        Args:
            figure_element: figure å…ƒç´ 

        Returns:
            åŒ…å« caption å’Œ image_src çš„å­—å…¸
        """
        try:
            caption_parts = []
            image_src = None

            # æå– figcaption
            figcaption = figure_element.locator('figcaption')
            if await figcaption.count() > 0:
                figcaption_text = await self.extract_html_with_formatting(figcaption.first)
                if figcaption_text.strip():
                    caption_parts.append(figcaption_text.strip())

            # æå– p.boldï¼ˆåœ–ç‰‡æ¨™é¡Œï¼‰
            bold_p = figure_element.locator('p.bold')
            if await bold_p.count() > 0:
                bold_text = await self.extract_html_with_formatting(bold_p.first)
                if bold_text.strip():
                    caption_parts.append(bold_text.strip())

            # æå–åœ–ç‰‡ src
            img = figure_element.locator('img')
            if await img.count() > 0:
                image_src = await img.first.get_attribute('src')

            if image_src:
                # åˆä½µæ‰€æœ‰èªªæ˜æ–‡å­—
                full_caption = ' - '.join(caption_parts) if caption_parts else 'åœ–ç‰‡'

                return {
                    'caption': full_caption,
                    'image_src': image_src,
                    'image_alt': full_caption
                }

            return None

        except Exception as e:
            logger.info(f"         âš ï¸  æå– figure å…§å®¹å¤±æ•—: {e}")
            return None

    async def _extract_container_content(self, container_element) -> list:
        """
        å¾ div[class^="container"] å…ƒç´ ä¸­æŒ‰é †åºæå–åœ–ç‰‡å’Œèªªæ˜æ–‡å­—
        
        æ”¯æŒå¤šç¨®æ ¼å¼è®Šé«”ï¼š
        - <div class="container">ã€<div class="container2">ã€<div class="container3"> ç­‰
        - <p class="caption">ã€<p class="caption2"> ç­‰ï¼ˆä»»ä½•åŒ…å« "caption" çš„ classï¼‰
        
        è™•ç†æ ¼å¼å¦‚ï¼š
        <div class="container2">
            <div id="_idContainer019">
                <img class="fit" src="image/p0018a.jpg" alt="" draggable="false">
                <p class="caption ...">ç²¾ç¾çš„æ—¥æœ¬ç¹ªç•«å±é¢¨...</p>
            </div>
        </div>
        
        Args:
            container_element: div[class^="container"] å…ƒç´ 
            
        Returns:
            å…§å®¹é …ç›®åˆ—è¡¨ï¼ˆæŒ‰ DOM é †åºï¼‰
        """
        try:
            result_items = []
            
            # æŸ¥æ‰¾æ‰€æœ‰å­å…ƒç´ ï¼ˆimg å’Œ pï¼ŒæŒ‰ DOM é †åºï¼‰
            children = container_element.locator('img, p')
            child_count = await children.count()
            
            for i in range(child_count):
                child = children.nth(i)
                tag_name = await child.evaluate('el => el.tagName.toLowerCase()')
                
                if tag_name == 'img':
                    # è™•ç†åœ–ç‰‡
                    src = await child.get_attribute('src')
                    alt = await child.get_attribute('alt') or 'åœ–ç‰‡'
                    element_class = await child.get_attribute('class') or ''
                    
                    if src:
                        result_items.append({
                            'type': 'image',
                            'image_src': src,
                            'image_alt': alt,
                            'image_class': element_class
                        })
                        
                elif tag_name == 'p':
                    # è™•ç†èªªæ˜æ–‡å­—ï¼ˆcaption, caption2, caption3 ç­‰ï¼‰
                    element_class = await child.get_attribute('class') or ''
                    text_content = await self.extract_html_with_formatting(child)
                    
                    if text_content.strip():
                        # å¦‚æœ class åŒ…å« "caption"ï¼Œä½œç‚ºåœ–ç‰‡èªªæ˜
                        # æ”¯æŒ: caption, caption2, caption3 ç­‰æ‰€æœ‰è®Šé«”
                        if 'caption' in element_class:
                            result_items.append({
                                'type': 'caption',
                                'content': text_content.strip()
                            })
                        else:
                            # ä¸€èˆ¬æ®µè½
                            result_items.append({
                                'type': 'p',
                                'content': text_content.strip()
                            })
            
            return result_items if result_items else None
            
        except Exception as e:
            logger.info(f"         âš ï¸  æå– container å…§å®¹å¤±æ•—: {e}")
            return None

    async def extract_chapter_name(self, iframe: FrameLocator) -> tuple:
        """
        å¾ iframe ä¸­æå–ç« ç¯€åç¨±å’Œæ’åºè™Ÿï¼ˆæ”¯æŒå¤šç¨®è¦å‰‡ï¼‰

        Args:
            iframe: iframe locator

        Returns:
            (ç« ç¯€åç¨±, æ’åºè™Ÿ, æ–‡ä»¶å, éŒ¨é»ID) çš„å…ƒçµ„
        """
        try:
            body = iframe.locator('body')
            import re

            # ä¸­æ–‡æ•¸å­—æ˜ å°„è¡¨
            chinese_nums = {
                'ä¸€': 1, 'äºŒ': 2, 'ä¸‰': 3, 'å››': 4, 'äº”': 5,
                'å…­': 6, 'ä¸ƒ': 7, 'å…«': 8, 'ä¹': 9, 'å': 10,
                'åä¸€': 11, 'åäºŒ': 12, 'åä¸‰': 13, 'åå››': 14, 'åäº”': 15,
                'åå…­': 16, 'åä¸ƒ': 17, 'åå…«': 18, 'åä¹': 19, 'äºŒå': 20
            }

            # æå–ç•¶å‰é é¢çš„æ–‡ä»¶åå’ŒéŒ¨é»ï¼ˆç”¨æ–¼èˆ‡ TOC åŒ¹é…ï¼‰
            current_file_name = None
            current_anchor_id = None
            
            try:
                base_element = iframe.locator('base').first
                base_href = await base_element.get_attribute('href')
                if base_href:
                    # å¾ base URL æå–æ–‡ä»¶å
                    # ä¾‹å¦‚ï¼š.../Text/ch-01.xhtml -> ch-01
                    match = re.search(r'([^/]+)\.xhtml', base_href)
                    if match:
                        current_file_name = match.group(1)
            except:
                pass

            # è¦å‰‡ 0: å„ªå…ˆæª¢æŸ¥ h1-h5 çš„ title å±¬æ€§ï¼ˆæœ€å®Œæ•´çš„ç« ç¯€åï¼‰
            for level in range(1, 6):  # h1 åˆ° h5
                elements = body.locator(f'h{level}[title]')
                count = await elements.count()

                if count > 0:
                    element = elements.first
                    title_attr = await element.get_attribute('title')
                    
                    if title_attr and title_attr.strip():
                        # åŒæ™‚å˜—è©¦æå– IDï¼ˆå¯èƒ½æœ‰ sigil_toc_idï¼‰
                        element_id = await element.get_attribute('id')
                        if element_id:
                            current_anchor_id = element_id
                            # å¾ ID æå–æ•¸å­—
                            match = re.search(r'sigil_toc_id_(\d+)', element_id)
                            if match:
                                order_num = int(match.group(1))
                                return (title_attr.strip(), order_num, current_file_name, current_anchor_id)
                        
                        # å˜—è©¦å¾ title æ–‡æœ¬ä¸­æå–æ•¸å­—
                        # åŒ¹é… "CHAPTER 1", "ç¬¬ä¸€ç« ", "1.1" ç­‰
                        chapter_match = re.search(r'CHAPTER\s+(\d+)', title_attr, re.IGNORECASE)
                        if chapter_match:
                            order_num = int(chapter_match.group(1))
                            return (title_attr.strip(), order_num, current_file_name, current_anchor_id)
                        
                        num_match = re.match(r'^(\d+(?:\.\d+)?)', title_attr.strip())
                        if num_match:
                            num_str = num_match.group(1)
                            try:
                                float_num = float(num_str)
                                order_num = int(float_num * 10)
                                return (title_attr.strip(), order_num, current_file_name, current_anchor_id)
                            except:
                                pass
                        
                        return (title_attr.strip(), None, current_file_name, current_anchor_id)

            # è¦å‰‡ 1: æª¢æŸ¥ h1-h5 çš„ sigil_toc_idï¼ˆå„ªå…ˆç´šæœ€é«˜ï¼‰
            for level in range(1, 6):  # h1 åˆ° h5
                elements = body.locator(f'h{level}[id^="sigil_toc_id_"]')
                count = await elements.count()

                if count > 0:
                    element = elements.first
                    element_id = await element.get_attribute('id')
                    element_text = await self.extract_html_with_formatting(element)
                    
                    if element_id:
                        current_anchor_id = element_id

                    # å¾ id ä¸­æå–æ•¸å­—
                    match = re.search(r'sigil_toc_id_(\d+)', element_id)
                    if match:
                        order_num = int(match.group(1))
                        return (element_text.strip(), order_num, current_file_name, current_anchor_id)

                    return (element_text.strip(), None, current_file_name, current_anchor_id)

            # è¦å‰‡ 2: æª¢æŸ¥ h1-h5 ä¸­çš„ span.num2 (Chapter X)
            for level in range(1, 6):  # h1 åˆ° h5
                elements = body.locator(f'h{level}')
                count = await elements.count()

                for i in range(count):
                    element = elements.nth(i)
                    span_num2 = element.locator('span.num2')

                    if await span_num2.count() > 0:
                        # ç²å–æ•´å€‹æ¨™é¡Œçš„æ–‡å­—ä½œç‚ºç« ç¯€å
                        chapter_name = await self.extract_html_with_formatting(element)
                        
                        # å˜—è©¦æå– ID
                        element_id = await element.get_attribute('id')
                        if element_id:
                            current_anchor_id = element_id

                        # å˜—è©¦å¾ span.num2 ä¸­æå–ç« ç¯€è™Ÿ
                        span_text = await span_num2.text_content()
                        match = re.search(r'Chapter\s+(\d+)', span_text, re.IGNORECASE)
                        if match:
                            order_num = int(match.group(1))
                            return (chapter_name.strip(), order_num, current_file_name, current_anchor_id)

                        return (chapter_name.strip(), None, current_file_name, current_anchor_id)

            # è¦å‰‡ 3: æª¢æŸ¥ h1-h5 ä¸­çš„ span.num (ç¬¬Xç« )
            for level in range(1, 6):  # h1 åˆ° h5
                elements = body.locator(f'h{level}')
                count = await elements.count()

                for i in range(count):
                    element = elements.nth(i)
                    span_num = element.locator('span.num')

                    if await span_num.count() > 0:
                        # ç²å–æ•´å€‹æ¨™é¡Œçš„æ–‡å­—ä½œç‚ºç« ç¯€å
                        chapter_name = await self.extract_html_with_formatting(element)
                        
                        # å˜—è©¦æå– ID
                        element_id = await element.get_attribute('id')
                        if element_id:
                            current_anchor_id = element_id

                        # å˜—è©¦å¾ span.num ä¸­æå–ç« ç¯€è™Ÿ
                        span_text = await span_num.text_content()

                        # å˜—è©¦åŒ¹é…ã€Œç¬¬Xç« ã€
                        match = re.search(r'ç¬¬([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+)ç« ', span_text)
                        if match:
                            num_str = match.group(1)
                            if num_str in chinese_nums:
                                order_num = chinese_nums[num_str]
                                return (chapter_name.strip(), order_num, current_file_name, current_anchor_id)
                            elif num_str.isdigit():
                                order_num = int(num_str)
                                return (chapter_name.strip(), order_num, current_file_name, current_anchor_id)

                        return (chapter_name.strip(), None, current_file_name, current_anchor_id)

            # è¦å‰‡ 4: æª¢æŸ¥ h1-h5 class="__reader-paragraph-spacing__"ï¼ˆå¦‚ "1.1 åˆä½œçš„æ¼”é€²"ï¼‰
            for level in range(1, 6):  # h1 åˆ° h5
                elements = body.locator(f'h{level}.__reader-paragraph-spacing__')
                count = await elements.count()

                if count > 0:
                    element = elements.first
                    chapter_name = await self.extract_html_with_formatting(element)
                    
                    # å˜—è©¦æå– ID
                    element_id = await element.get_attribute('id')
                    if element_id:
                        current_anchor_id = element_id
                    
                    # å˜—è©¦å¾ç« ç¯€åç¨±ä¸­æå–æ•¸å­—ç·¨è™Ÿï¼ˆå¦‚ "1.1", "2.3", "10.5"ï¼‰
                    match = re.match(r'^(\d+(?:\.\d+)?)', chapter_name.strip())
                    if match:
                        num_str = match.group(1)
                        # å°‡ "1.1" è½‰æ›ç‚º 1.1ï¼ˆæµ®é»æ•¸ï¼‰ç„¶å¾Œä¹˜ä»¥ 10 å¾—åˆ°æ•´æ•¸æ’åº
                        # ä¾‹å¦‚ï¼š1.1 -> 11, 2.3 -> 23, 10.5 -> 105
                        try:
                            float_num = float(num_str)
                            order_num = int(float_num * 10)
                            return (chapter_name.strip(), order_num, current_file_name, current_anchor_id)
                        except:
                            pass
                    
                    # å˜—è©¦åŒ¹é…å–®ç´”çš„æ•¸å­—é–‹é ­ï¼ˆå¦‚ "1 å‰è¨€"ï¼‰
                    match = re.match(r'^(\d+)\s+', chapter_name.strip())
                    if match:
                        order_num = int(match.group(1))
                        return (chapter_name.strip(), order_num, current_file_name, current_anchor_id)
                    
                    # æ²’æœ‰æ‰¾åˆ°æ•¸å­—ï¼Œä½†æœ‰ç« ç¯€å
                    return (chapter_name.strip(), None, current_file_name, current_anchor_id)

            # è¦å‰‡ 5: æª¢æŸ¥ p.titlebig ä½œç‚ºç« ç¯€å
            p_titlebig = body.locator('p.titlebig')
            if await p_titlebig.count() > 0:
                chapter_name = await self.extract_html_with_formatting(p_titlebig.first)
                
                # å˜—è©¦å¾æ–‡å­—ä¸­æå–æ•¸å­—
                match = re.match(r'^(\d+(?:\.\d+)?)', chapter_name.strip())
                if match:
                    num_str = match.group(1)
                    try:
                        float_num = float(num_str)
                        order_num = int(float_num * 10)
                        return (chapter_name.strip(), order_num, current_file_name, current_anchor_id)
                    except:
                        pass
                
                return (chapter_name.strip(), None, current_file_name, current_anchor_id)

            # å‚™ç”¨æ–¹æ¡ˆï¼šå˜—è©¦æ‰¾ç¬¬ä¸€å€‹ h1-h5
            for level in range(1, 6):  # h1 åˆ° h5
                elements = body.locator(f'h{level}')
                if await elements.count() > 0:
                    first_heading = await self.extract_html_with_formatting(elements.first)
                    element_id = await elements.first.get_attribute('id')
                    if element_id:
                        current_anchor_id = element_id
                    return (first_heading.strip(), None, current_file_name, current_anchor_id)

            return ("", None, None, None)

        except Exception as e:
            logger.info(f"         âš ï¸  æå–ç« ç¯€åç¨±å¤±æ•—: {e}")
            return ("", None, None, None)

    async def is_toc_page(self, iframe: FrameLocator) -> bool:
        """
        åˆ¤æ–·æ˜¯å¦ç‚ºç›®éŒ„é ï¼ˆæ”¯æŒå¤šç¨®æ ¼å¼ï¼‰

        Args:
            iframe: iframe locator

        Returns:
            æ˜¯å¦ç‚ºç›®éŒ„é 
        """
        try:
            body = iframe.locator('body')

            # æª¢æŸ¥ 1: æ˜¯å¦æœ‰ nav[epub:type="toc"]
            toc_nav = body.locator('nav[epub\\:type="toc"]')
            if await toc_nav.count() > 0:
                return True

            # æª¢æŸ¥ 2: body æ˜¯å¦æœ‰ class="p-toc" æˆ–é¡ä¼¼çš„ç›®éŒ„æ¨™è¨˜
            body_class = await body.get_attribute('class')
            if body_class and ('toc' in body_class.lower() or 'contents' in body_class.lower()):
                return True

            # æª¢æŸ¥ 3: h1 æ˜¯å¦åŒ…å«ã€Œç›®éŒ„ã€
            h1_elements = body.locator('h1')
            if await h1_elements.count() > 0:
                h1_text = await h1_elements.first.text_content()
                if h1_text and 'ç›®éŒ„' in h1_text:
                    return True

            # æª¢æŸ¥ 4: div æ˜¯å¦åŒ…å«ã€Œç›®éŒ„ã€æ–‡å­—ï¼ˆæ–°æ ¼å¼ï¼‰
            div_elements = body.locator('div:has-text("ç›®éŒ„")')
            if await div_elements.count() > 0:
                # æª¢æŸ¥æ˜¯å¦æœ‰è¶³å¤ çš„éˆæ¥ï¼ˆè‡³å°‘ 3 å€‹ï¼‰
                links = body.locator('a[href*=".xhtml"]')
                if await links.count() >= 3:
                    return True

            return False
        except:
            return False

    async def extract_toc_links(self, iframe: FrameLocator) -> list:
        """
        å¾ç›®éŒ„é æå–æ‰€æœ‰ç« ç¯€éˆæ¥ï¼ˆå¸¶ç´¢å¼•è™Ÿï¼Œæ”¯æŒå¤šç¨®æ ¼å¼ï¼‰

        Args:
            iframe: iframe locator

        Returns:
            ç« ç¯€éˆæ¥åˆ—è¡¨ [{'title': 'ç« ç¯€æ¨™é¡Œ', 'href': 'éˆæ¥', 'toc_index': ç´¢å¼•è™Ÿ, 'level': å±¤ç´š}]
        """
        try:
            toc_items = []
            body = iframe.locator('body')
            import re

            # æ–¹æ³• 1: æ¨™æº– EPUB æ ¼å¼ï¼ˆnav[epub:type="toc"]ï¼‰
            nav_links = body.locator('nav[epub\\:type="toc"] a, ol a, ul a')
            nav_count = await nav_links.count()

            if nav_count > 0:
                logger.info(f"         ğŸ“š ä½¿ç”¨æ¨™æº– EPUB TOC æ ¼å¼")
                for i in range(nav_count):
                    link = nav_links.nth(i)
                    title = await link.text_content()
                    href = await link.get_attribute('href')

                    if title and href:
                        # æå–æ–‡ä»¶åï¼ˆä¸åŒ…å«éŒ¨é»ï¼‰
                        match = re.search(r'([^/]+)\.xhtml', href)
                        file_name = match.group(1) if match else None
                        
                        # æå–éŒ¨é» ID
                        anchor_match = re.search(r'#(.+)$', href)
                        anchor_id = anchor_match.group(1) if anchor_match else None
                        
                        toc_items.append({
                            'title': title.strip(),
                            'href': href,
                            'file_name': file_name,
                            'anchor_id': anchor_id,
                            'toc_index': i,
                            'level': 0  # æ¨™æº–æ ¼å¼ä¸å€åˆ†å±¤ç´š
                        })

            # æ–¹æ³• 2: ç°¡åŒ–æ ¼å¼ï¼ˆbody.p-toc æˆ–åŒ…å«"ç›®éŒ„"çš„ divï¼‰
            else:
                logger.info(f"         ğŸ“– ä½¿ç”¨ç°¡åŒ– TOC æ ¼å¼")
                
                # æ‰¾åˆ°æ‰€æœ‰åŒ…å« .xhtml éˆæ¥çš„ <a> æ¨™ç±¤
                all_links = body.locator('a[href*=".xhtml"]')
                link_count = await all_links.count()

                for i in range(link_count):
                    link = all_links.nth(i)
                    title = await link.text_content()
                    href = await link.get_attribute('href')

                    if not title or not href:
                        continue

                    # æå–æ–‡ä»¶å
                    match = re.search(r'([^/]+)\.xhtml', href)
                    file_name = match.group(1) if match else None
                    
                    # æå–éŒ¨é» ID
                    anchor_match = re.search(r'#(.+)$', href)
                    anchor_id = anchor_match.group(1) if anchor_match else None
                    
                    # åˆ¤æ–·å±¤ç´šï¼ˆé€šéçˆ¶å…ƒç´ çš„ classï¼‰
                    level = 0
                    try:
                        # æª¢æŸ¥çˆ¶å…ƒç´ æ˜¯å¦æœ‰ç¸®é€² classï¼ˆå¦‚ start-4em50ï¼‰
                        parent_p = link.locator('xpath=ancestor::p[1]')
                        if await parent_p.count() > 0:
                            parent_div = parent_p.locator('xpath=parent::div[1]')
                            if await parent_div.count() > 0:
                                parent_class = await parent_div.first.get_attribute('class')
                                if parent_class:
                                    # è­˜åˆ¥ç¸®é€² classï¼ˆstart-4em50, start-2em ç­‰ï¼‰
                                    if 'start-4em' in parent_class or 'start-3em' in parent_class:
                                        level = 2  # å­ç« ç¯€
                                    elif 'start-2em' in parent_class:
                                        level = 1  # æ¬¡ç´šç« ç¯€
                    except:
                        pass
                    
                    # æ¸…ç†æ¨™é¡Œï¼ˆç§»é™¤å¤šé¤˜ç©ºæ ¼å’Œæ›è¡Œï¼‰
                    clean_title = re.sub(r'\s+', ' ', title.strip())
                    
                    toc_items.append({
                        'title': clean_title,
                        'href': href,
                        'file_name': file_name,
                        'anchor_id': anchor_id,
                        'toc_index': i,
                        'level': level  # 0=ä¸»ç« ç¯€, 1=æ¬¡ç´š, 2=å­ç« ç¯€
                    })

            logger.info(f"         ğŸ“‘ æå–åˆ° {len(toc_items)} å€‹ç›®éŒ„é …")
            
            # Debug: é¡¯ç¤ºå‰ 5 å€‹é …ç›®
            if toc_items:
                logger.info(f"         ğŸ“‹ ç›®éŒ„é è¦½ï¼ˆå‰5é …ï¼‰ï¼š")
                for item in toc_items[:5]:
                    indent = "  " * item.get('level', 0)
                    logger.info(f"            {indent}[{item['toc_index']}] {item['title']}")
            
            return toc_items

        except Exception as e:
            logger.info(f"         âš ï¸  æå–ç›®éŒ„éˆæ¥å¤±æ•—: {e}")
            import traceback
            traceback.print_exc()
            return []

    async def scrape_chapter_from_iframe(self, iframe: FrameLocator, base_url: str = None, toc_links: list = None) -> Dict[str, any]:
        """
        å¾å–®å€‹ iframe æŠ“å–å®Œæ•´ç« ç¯€å…§å®¹ï¼ˆä¿æŒå…ƒç´ é †åºï¼Œæ”¯æŒ TOC æ™ºèƒ½åŒ¹é…ï¼‰

        Args:
            iframe: iframe locator
            base_url: åŸºç¤ URLï¼ˆç”¨æ–¼è§£æåœ–ç‰‡ç›¸å°è·¯å¾‘ï¼‰
            toc_links: TOC ç›®éŒ„éˆæ¥åˆ—è¡¨ï¼ˆç”¨æ–¼æ™ºèƒ½æ’åºï¼‰

        Returns:
            ç« ç¯€è³‡æ–™å­—å…¸ï¼ŒåŒ…å«ç« ç¯€åå’Œæœ‰åºå…§å®¹åˆ—è¡¨
        """
        try:
            # æª¢æŸ¥æ˜¯å¦ç‚ºç›®éŒ„é 
            is_toc = await self.is_toc_page(iframe)

            # æå–ç« ç¯€åç¨±ã€æ’åºè™Ÿã€æ–‡ä»¶åã€éŒ¨é»ID
            chapter_name, order_num, file_name, anchor_id = await self.extract_chapter_name(iframe)
            
            # ğŸ” æ™ºèƒ½ TOC åŒ¹é…ï¼šä½¿ç”¨ TOC æä¾›æ›´æº–ç¢ºçš„æ’åº
            toc_index = None
            toc_title = None
            
            if toc_links and (file_name or anchor_id or chapter_name):
                # ç­–ç•¥1: ç²¾ç¢ºåŒ¹é…ï¼ˆæ–‡ä»¶å + éŒ¨é»IDï¼‰
                if file_name and anchor_id:
                    for toc_item in toc_links:
                        if toc_item['file_name'] == file_name and toc_item.get('anchor_id') == anchor_id:
                            toc_index = toc_item['toc_index']
                            toc_title = toc_item['title']
                            logger.info(f"         ğŸ¯ TOC ç²¾ç¢ºåŒ¹é…: [{toc_index}] {toc_title}")
                            break
                
                # ç­–ç•¥2: æ–‡ä»¶ååŒ¹é…ï¼ˆç„¡éŒ¨é»ï¼‰
                if toc_index is None and file_name:
                    for toc_item in toc_links:
                        if toc_item['file_name'] == file_name and not toc_item.get('anchor_id'):
                            toc_index = toc_item['toc_index']
                            toc_title = toc_item['title']
                            logger.info(f"         ğŸ“ TOC æ–‡ä»¶ååŒ¹é…: [{toc_index}] {toc_title}")
                            break
                
                # ç­–ç•¥3: ç« ç¯€åæ¨¡ç³ŠåŒ¹é…ï¼ˆæ–‡å­—ç›¸ä¼¼åº¦ï¼‰
                if toc_index is None and chapter_name:
                    best_match_score = 0
                    best_match_item = None
                    
                    for toc_item in toc_links:
                        toc_item_title = toc_item['title']
                        
                        # è¨ˆç®—ç›¸ä¼¼åº¦ï¼ˆç°¡å–®çš„åŒ…å«é—œä¿‚ï¼‰
                        if chapter_name in toc_item_title or toc_item_title in chapter_name:
                            # ç²¾ç¢ºåŒ…å«
                            score = 0.9
                        elif chapter_name.replace(' ', '') in toc_item_title.replace(' ', ''):
                            # å»ç©ºæ ¼å¾ŒåŒ…å«
                            score = 0.8
                        else:
                            # è¨ˆç®—å…±åŒå­—ç¬¦æ•¸
                            common_chars = sum(1 for c in chapter_name if c in toc_item_title)
                            score = common_chars / max(len(chapter_name), len(toc_item_title))
                        
                        if score > best_match_score and score > 0.6:  # è‡³å°‘ 60% ç›¸ä¼¼åº¦
                            best_match_score = score
                            best_match_item = toc_item
                    
                    if best_match_item:
                        toc_index = best_match_item['toc_index']
                        toc_title = best_match_item['title']
                        logger.info(f"         ğŸ’¡ TOC æ¨¡ç³ŠåŒ¹é…: [{toc_index}] {toc_title} (ç›¸ä¼¼åº¦: {best_match_score:.1%})")
            
            # å„ªå…ˆä½¿ç”¨ TOC ç´¢å¼•ï¼Œå¦å‰‡ä½¿ç”¨ extract_chapter_name çš„ order_num
            if toc_index is not None:
                order_num = toc_index  # TOC ç´¢å¼•å„ªå…ˆ
                if toc_title and not chapter_name:
                    chapter_name = toc_title  # å¦‚æœæ²’æœ‰ç« ç¯€åï¼Œä½¿ç”¨ TOC æ¨™é¡Œ

            if not chapter_name:
                # å¦‚æœæ²’æœ‰ç« ç¯€åï¼Œä½¿ç”¨ç‰¹æ®Šæ¨™è¨˜ï¼ˆå¯èƒ½æ˜¯å°é¢æˆ–å‰è¨€ï¼‰
                chapter_name = "__no_chapter__"
                order_num = None

            # å¦‚æœæ˜¯ç›®éŒ„é ï¼Œæå–ç›®éŒ„éˆæ¥
            toc_links = []
            if is_toc or 'ç›®éŒ„' in chapter_name:
                toc_links = await self.extract_toc_links(iframe)
                if toc_links:
                    chapter_name = "ç›®éŒ„"  # çµ±ä¸€å‘½åç‚ºã€Œç›®éŒ„ã€
                    order_num = None  # ç›®éŒ„ä¸åƒèˆ‡æ’åº

            # æŒ‰é †åºæŠ“å–æ‰€æœ‰å…§å®¹å…ƒç´ ï¼ˆä¿æŒ DOM é †åºï¼‰
            content_items = []

            # æŠ“å– body å…§çš„æ‰€æœ‰å…ƒç´ 
            body = iframe.locator('body')

            # ä¸€æ¬¡æ€§æŠ“å–æ‰€æœ‰å…§å®¹å…ƒç´ ä¸¦ä¿æŒé †åº
            # é‡è¦ï¼šæ’é™¤ div[class^="container"] å’Œ figure å…§éƒ¨çš„ p, imgï¼Œé¿å…é‡è¤‡è™•ç†
            # é€™äº›å…ƒç´ æœƒç”±å°ˆé–€çš„ _extract_container_content å’Œ _extract_figure_content è™•ç†
            all_elements = body.locator(
                'h1:not(div[class^="container"] *, figure *), '
                'h2:not(div[class^="container"] *, figure *), '
                'h3:not(div[class^="container"] *, figure *), '
                'h4:not(div[class^="container"] *, figure *), '
                'h5:not(div[class^="container"] *, figure *), '
                'h6:not(div[class^="container"] *, figure *), '
                'p:not(div[class^="container"] *, figure *), '
                'figure, '
                'div[class^="container"]'
            )
            element_count = await all_elements.count()

            for i in range(element_count):
                element = all_elements.nth(i)

                # ç²å–å…ƒç´ çš„æ¨™ç±¤å
                tag_name = await element.evaluate('el => el.tagName.toLowerCase()')

                if tag_name == 'figure':
                    # è™•ç† figure å…ƒç´ ï¼ˆåœ–ç‰‡ + èªªæ˜æ–‡å­—ï¼‰
                    figure_data = await self._extract_figure_content(element)
                    if figure_data:
                        # å°‡ figure ä½œç‚ºç‰¹æ®Šçš„å…§å®¹é …ç›®
                        content_items.append({
                            'type': 'figure',
                            'content': figure_data['caption'],
                            'image_src': figure_data['image_src'],
                            'image_alt': figure_data['image_alt']
                        })
                elif tag_name == 'div':
                    # è™•ç† div[class^="container"] å…§çš„åœ–ç‰‡å’Œèªªæ˜æ–‡å­—ï¼ˆæŒ‰é †åºï¼‰
                    # æ”¯æŒ container, container2, container3 ç­‰æ‰€æœ‰è®Šé«”
                    container_data = await self._extract_container_content(element)
                    if container_data:
                        for item in container_data:
                            content_items.append(item)
                else:
                    # ç²å–å…ƒç´ çš„æ–‡å­—å…§å®¹ï¼ˆä¿ç•™æ ¼å¼ï¼‰
                    text_content = await self.extract_html_with_formatting(element)

                    if text_content.strip():
                        # æª¢æŸ¥æ˜¯å¦æœ‰ç‰¹æ®Š class éœ€è¦è™•ç†
                        element_class = await element.get_attribute('class') or ''
                        epub_type = await element.get_attribute('epub:type') or ''
                        
                        # è™•ç†ç‰¹æ®Šæ¨£å¼é¡
                        final_content = text_content.strip()
                        
                        # footnote é¡ï¼šè…³è¨»ï¼Œæ¨™è¨˜ç‚º footnote
                        if 'footnote' in element_class or epub_type == 'footnote':
                            # æå–è…³è¨»ç·¨è™Ÿï¼ˆå¾ <a> æ¨™ç±¤å…§å®¹ï¼‰
                            footnote_num = await element.locator('a').first.text_content() if await element.locator('a').count() > 0 else ''
                            if footnote_num.strip():
                                final_content = f"[^{footnote_num.strip()}]: {final_content}"
                            else:
                                final_content = f"**[è¨»]** {final_content}"
                        # titlebig é¡ï¼šå¤§æ¨™é¡Œï¼ŒåŠ ç²—é«”
                        elif 'titlebig' in element_class:
                            final_content = f"**{final_content}**"
                        # titlemid é¡ï¼šä¸­ç­‰æ¨™é¡Œï¼ŒåŠ ç²—é«”
                        elif 'titlemid' in element_class:
                            final_content = f"**{final_content}**"
                        
                        content_items.append({
                            'type': tag_name,
                            'content': final_content
                        })

            # æŠ“å–ä¸åœ¨ figure å…§çš„ç¨ç«‹åœ–ç‰‡
            # æ³¨æ„ï¼šé€™è£¡åŒ…æ‹¬ container å…§çš„åœ–ç‰‡ï¼Œç”¨æ–¼ä¸‹è¼‰ï¼Œä½†åœ¨ Markdown è¼¸å‡ºæ™‚æœƒå»é‡
            images = []

            # ä¸€èˆ¬åœ–ç‰‡ï¼ˆæ’é™¤ figure å…§çš„ï¼‰
            img_elements = body.locator('img:not(figure img)')
            img_count = await img_elements.count()

            for i in range(img_count):
                img = img_elements.nth(i)
                src = await img.get_attribute('src')
                alt = await img.get_attribute('alt') or 'åœ–ç‰‡'

                if src:
                    images.append({
                        'src': src,
                        'alt': alt
                    })

            # SVG åœ–ç‰‡ï¼ˆæ’é™¤ figure å…§çš„ï¼‰
            svg_images = body.locator('svg:not(figure svg) image')
            svg_count = await svg_images.count()

            for i in range(svg_count):
                svg_img = svg_images.nth(i)

                # å„ªå…ˆå˜—è©¦ xlink:href
                src = await svg_img.get_attribute('xlink:href')
                if not src:
                    src = await svg_img.get_attribute('href')

                if src:
                    images.append({
                        'src': src,
                        'alt': 'SVG åœ–ç‰‡'
                    })

            # Canvas åœ–ç‰‡ï¼ˆæ’é™¤ figure å…§çš„ï¼‰
            canvas_elements = body.locator('canvas:not(figure canvas)')
            canvas_count = await canvas_elements.count()

            if canvas_count > 0:
                logger.info(f"         ğŸ¨ æ‰¾åˆ° {canvas_count} å€‹ Canvas å…ƒç´ ")

            for i in range(canvas_count):
                canvas = canvas_elements.nth(i)
                
                try:
                    # ç­‰å¾… Canvas æ¸²æŸ“å®Œæˆï¼ˆæª¢æŸ¥æ˜¯å¦æœ‰å…§å®¹ï¼‰
                    # æœ€å¤šç­‰å¾… 3 ç§’ï¼Œæ¯ 0.5 ç§’æª¢æŸ¥ä¸€æ¬¡
                    canvas_ready = False
                    for attempt in range(6):
                        has_content = await canvas.evaluate('''
                            canvas => {
                                try {
                                    const ctx = canvas.getContext('2d');
                                    if (!ctx) return false;
                                    
                                    // æª¢æŸ¥ canvas æ˜¯å¦æœ‰å…§å®¹ï¼ˆä¸æ˜¯å®Œå…¨ç©ºç™½ï¼‰
                                    const imageData = ctx.getImageData(0, 0, canvas.width, canvas.height);
                                    const data = imageData.data;
                                    
                                    // æª¢æŸ¥æ˜¯å¦æœ‰éé€æ˜çš„åƒç´ 
                                    for (let i = 3; i < data.length; i += 4) {
                                        if (data[i] > 0) {
                                            return true;  // æ‰¾åˆ°éé€æ˜åƒç´ 
                                        }
                                    }
                                    return false;
                                } catch (e) {
                                    return false;
                                }
                            }
                        ''')
                        
                        if has_content:
                            canvas_ready = True
                            logger.info(f"         âœ“ Canvas[{i}] å·²æ¸²æŸ“å®Œæˆï¼ˆå˜—è©¦ {attempt + 1} æ¬¡ï¼‰")
                            break
                        
                        if attempt < 5:
                            await asyncio.sleep(0.5)
                    
                    if not canvas_ready:
                        logger.info(f"         âš ï¸  Canvas[{i}] å¯èƒ½ç‚ºç©ºæˆ–æœªæ¸²æŸ“å®Œæˆ")
                        # ä»ç„¶å˜—è©¦æŠ“å–ï¼Œå¯èƒ½æœ‰å…§å®¹åªæ˜¯æª¢æ¸¬å¤±æ•—
                    
                    # å°‡ canvas è½‰æ›ç‚º data URLï¼ˆPNG æ ¼å¼ï¼‰
                    data_url = await canvas.evaluate('''
                        canvas => {
                            try {
                                return canvas.toDataURL('image/png');
                            } catch (e) {
                                console.error('Canvas toDataURL error:', e);
                                return null;
                            }
                        }
                    ''')
                    
                    if data_url and data_url.startswith('data:image'):
                        # æª¢æŸ¥ data URL çš„å¤§å°ï¼ˆæ’é™¤éå°çš„ç©ºç™½åœ–ç‰‡ï¼‰
                        data_size = len(data_url)
                        
                        # ç©ºç™½çš„ PNG é€šå¸¸å¾ˆå°ï¼ˆ< 1KBï¼‰ï¼Œå¯¦éš›å…§å®¹é€šå¸¸ > 5KB
                        if data_size > 5000:
                            images.append({
                                'src': data_url,
                                'alt': f'Canvas åœ–ç‰‡ {i+1}',
                                'is_canvas': True  # æ¨™è¨˜ç‚º canvas åœ–ç‰‡
                            })
                            logger.info(f"         âœ… Canvas[{i}] å·²è½‰æ›ç‚ºåœ–ç‰‡ ({data_size / 1024:.1f} KB)")
                        else:
                            logger.info(f"         âš ï¸  Canvas[{i}] åœ–ç‰‡éå° ({data_size} bytes)ï¼Œå¯èƒ½ç‚ºç©ºç™½")
                    else:
                        logger.info(f"         âš ï¸  Canvas[{i}] è½‰æ›å¤±æ•—æˆ–ç‚ºç©º")
                        
                except Exception as e:
                    logger.info(f"         âš ï¸  Canvas[{i}] æŠ“å–å¤±æ•—: {e}")

            # æŠ“å–è¨»é‡‹
            footnotes = []
            footnote_elements = body.locator('div.footnote[role="doc-endnote"]')
            footnote_count = await footnote_elements.count()

            if footnote_count > 0:
                for i in range(footnote_count):
                    footnote = footnote_elements.nth(i)
                    footnote_ps = footnote.locator('p')
                    p_count = await footnote_ps.count()

                    for j in range(p_count):
                        p_text = await self.extract_html_with_formatting(footnote_ps.nth(j))
                        if p_text.strip():
                            footnotes.append(p_text.strip())

            # æ”¶é›† figure ä¸­çš„åœ–ç‰‡
            figure_images = []
            for item in content_items:
                if item['type'] == 'figure' and 'image_src' in item:
                    figure_images.append({
                        'src': item['image_src'],
                        'alt': item['image_alt']
                    })

            return {
                'name': chapter_name,
                'order_num': order_num,  # ç« ç¯€æ’åºè™Ÿ
                'content_items': content_items,
                'images': images,
                'figure_images': figure_images,  # figure ä¸­çš„åœ–ç‰‡
                'footnotes': footnotes,
                'is_toc': is_toc or 'ç›®éŒ„' in chapter_name,  # æ˜¯å¦ç‚ºç›®éŒ„é 
                'toc_links': toc_links  # ç›®éŒ„éˆæ¥åˆ—è¡¨
            }

        except Exception as e:
            logger.info(f"         âš ï¸  å¾ iframe æŠ“å–ç« ç¯€æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return None

    async def _scrape_from_single_iframe(self, iframe: FrameLocator) -> Dict[str, any]:
        """
        å¾å–®å€‹ iframe æŠ“å–å…§å®¹ï¼ˆèˆŠç‰ˆæœ¬ï¼Œä¿ç•™å‘å¾Œå…¼å®¹ï¼‰

        Args:
            iframe: iframe locator

        Returns:
            åŒ…å«æ¨™é¡Œã€æ®µè½å’Œåœ–ç‰‡çš„å­—å…¸
        """
        content = {
            'headings': [],
            'paragraphs': [],
            'images': []
        }

        try:

            # æŠ“å–æ¨™é¡Œ (h1, h2, h3, h4, h5, h6)
            for tag in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:
                elements = iframe.locator(tag)
                count = await elements.count()

                for i in range(count):
                    # ä½¿ç”¨æ–°æ–¹æ³•æå–åŒ…å«æ ¼å¼çš„æ–‡å­—
                    text = await self.extract_html_with_formatting(elements.nth(i))
                    if text and text.strip():
                        content['headings'].append({
                            'level': tag,
                            'text': text.strip()
                        })

            # æŠ“å–æ®µè½ï¼ˆåŒ…å«ä¸€èˆ¬æ®µè½å’Œè…³è¨»ï¼‰
            paragraphs = iframe.locator('p')
            p_count = await paragraphs.count()

            for i in range(p_count):
                # ä½¿ç”¨æ–°æ–¹æ³•æå–åŒ…å«æ ¼å¼çš„æ–‡å­—
                text = await self.extract_html_with_formatting(paragraphs.nth(i))
                if text and text.strip():
                    content['paragraphs'].append(text.strip())

            # é¡å¤–æŠ“å– footnoteï¼ˆè…³è¨»ï¼‰
            footnotes = iframe.locator('.footnote[role="doc-endnote"]')
            footnote_count = await footnotes.count()

            if footnote_count > 0:
                content['paragraphs'].append('\n---\n\n**è¨»é‡‹ï¼š**\n')

                for i in range(footnote_count):
                    footnote = footnotes.nth(i)
                    # ç²å– footnote å…§çš„æ‰€æœ‰æ®µè½
                    fn_paragraphs = footnote.locator('p')
                    fn_p_count = await fn_paragraphs.count()

                    for j in range(fn_p_count):
                        text = await self.extract_html_with_formatting(fn_paragraphs.nth(j))
                        if text and text.strip():
                            content['paragraphs'].append(text.strip())

            # æŠ“å–åœ–ç‰‡ (HTML img æ¨™ç±¤)
            images = iframe.locator('img')
            img_count = await images.count()

            for i in range(img_count):
                src = await images.nth(i).get_attribute('src')
                alt = await images.nth(i).get_attribute('alt')
                if src:
                    content['images'].append({
                        'src': src,
                        'alt': alt or ''
                    })

            # æŠ“å–åœ–ç‰‡ (SVG image æ¨™ç±¤)
            svg_images = iframe.locator('image')
            svg_img_count = await svg_images.count()

            for i in range(svg_img_count):
                # SVG ä½¿ç”¨ xlink:href æˆ– href å±¬æ€§
                src = await svg_images.nth(i).get_attribute('xlink:href')
                if not src:
                    src = await svg_images.nth(i).get_attribute('href')

                if src:
                    # è™•ç†ç›¸å°è·¯å¾‘ï¼Œè½‰æ›ç‚ºçµ•å° URL
                    # ç²å– iframe çš„ base URL
                    try:
                        # å˜—è©¦å¾ iframe ç²å–å®Œæ•´ URL
                        base_element = iframe.locator('base').first
                        base_href = await base_element.get_attribute('href')

                        if base_href and src.startswith('../'):
                            # è™•ç†ç›¸å°è·¯å¾‘
                            # ../Images/cover.jpg -> å¾ base_href è¨ˆç®—å®Œæ•´è·¯å¾‘
                            full_url = urljoin(base_href, src)
                            src = full_url
                    except:
                        # å¦‚æœå¤±æ•—ï¼Œä¿æŒåŸæ¨£
                        pass

                    content['images'].append({
                        'src': src,
                        'alt': 'SVG åœ–ç‰‡'
                    })

            return content

        except Exception as e:
            logger.info(f"         âš ï¸  å¾ iframe æŠ“å–å…§å®¹æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return {'headings': [], 'paragraphs': [], 'images': []}

    async def download_image(self, url: str, page_number: int, base_url: str = None) -> str:
        """
        ä¸‹è¼‰åœ–ç‰‡åˆ°æœ¬åœ°

        Args:
            url: åœ–ç‰‡ URLï¼ˆå¯èƒ½æ˜¯ç›¸å°è·¯å¾‘æˆ– data URLï¼‰
            page_number: é ç¢¼
            base_url: åŸºç¤ URLï¼ˆç”¨æ–¼è§£æç›¸å°è·¯å¾‘ï¼‰

        Returns:
            æœ¬åœ°åœ–ç‰‡è·¯å¾‘ï¼ˆç›¸å°æ–¼ Markdown æª”æ¡ˆï¼‰
        """
        # æª¢æŸ¥æ˜¯å¦å·²ä¸‹è¼‰
        if url in self.downloaded_images:
            return self.downloaded_images[url]

        try:
            # è™•ç† data URLï¼ˆä¾‹å¦‚ Canvas ç”Ÿæˆçš„åœ–ç‰‡ï¼‰
            if url.startswith('data:image'):
                import base64
                
                # è§£æ data URL
                # æ ¼å¼: data:image/png;base64,iVBORw0KGgoAAAANS...
                match = re.match(r'data:image/(\w+);base64,(.+)', url)
                if match:
                    img_format = match.group(1)
                    img_data = match.group(2)
                    
                    # ç”Ÿæˆæª”æ¡ˆåç¨±
                    url_hash = hashlib.md5(url.encode()).hexdigest()[:8]
                    filename = f"page_{page_number:04d}_canvas_{url_hash}.{img_format}"
                    
                    local_path = self.images_dir / filename
                    
                    # è§£ç¢¼ä¸¦ä¿å­˜åœ–ç‰‡
                    with open(local_path, 'wb') as f:
                        f.write(base64.b64decode(img_data))
                    
                    # è¨˜éŒ„ä¸‹è¼‰
                    relative_path = f"images/book_{self.book_id}/{filename}"
                    self.downloaded_images[url] = relative_path
                    
                    logger.info(f"      ğŸ¨ å·²ä¿å­˜ Canvas åœ–ç‰‡: {filename}")
                    return relative_path
                else:
                    logger.info(f"      âš ï¸  ç„¡æ³•è§£æ data URL")
                    return url
            
            # è™•ç†ç›¸å°è·¯å¾‘
            download_url = url
            if not url.startswith(('http://', 'https://')):
                if base_url:
                    # ä½¿ç”¨ urljoin è½‰æ›ç›¸å°è·¯å¾‘ç‚ºçµ•å°è·¯å¾‘
                    download_url = urljoin(base_url, url)
                    logger.info(f"      ğŸ”— è½‰æ› URL: {url} -> {download_url}")
                else:
                    logger.info(f"      âš ï¸  ç„¡æ³•ä¸‹è¼‰ç›¸å°è·¯å¾‘åœ–ç‰‡ï¼ˆç¼ºå°‘ base_urlï¼‰: {url}")
                    return url

            # ç”Ÿæˆæª”æ¡ˆåç¨±ï¼ˆä½¿ç”¨ URL hash + é ç¢¼ï¼‰
            url_hash = hashlib.md5(url.encode()).hexdigest()[:8]
            ext = Path(url).suffix or '.jpg'
            filename = f"page_{page_number:04d}_{url_hash}{ext}"

            local_path = self.images_dir / filename

            # ä¸‹è¼‰åœ–ç‰‡
            async with httpx.AsyncClient(timeout=30.0, follow_redirects=True) as client:
                response = await client.get(download_url)
                response.raise_for_status()

                # ä¿å­˜åœ–ç‰‡
                with open(local_path, 'wb') as f:
                    f.write(response.content)

            # è¨˜éŒ„ä¸‹è¼‰ï¼ˆç›¸å°æ–¼ downloads ç›®éŒ„çš„è·¯å¾‘ï¼‰
            relative_path = f"images/book_{self.book_id}/{filename}"
            self.downloaded_images[url] = relative_path

            logger.info(f"      ğŸ“¥ å·²ä¸‹è¼‰åœ–ç‰‡: {filename}")
            return relative_path

        except Exception as e:
            logger.info(f"      âš ï¸  ä¸‹è¼‰åœ–ç‰‡å¤±æ•— ({url[:100]}...): {e}")
            # ä¸‹è¼‰å¤±æ•—æ™‚è¿”å›åŸ URL
            return url

    def extract_chapter_number(self, chapter_name: str, order_num: int = None) -> tuple:
        """
        å¾ç« ç¯€åç¨±ä¸­æå–ç« ç¯€ç·¨è™Ÿ

        Args:
            chapter_name: ç« ç¯€åç¨±
            order_num: å·²æå–çš„æ’åºè™Ÿï¼ˆå„ªå…ˆä½¿ç”¨ï¼‰

        Returns:
            (ç« ç¯€é¡å‹, ç« ç¯€ç·¨è™Ÿ)
            - ç« ç¯€é¡å‹: 'front' (å‰ç½®), 'main' (æ­£æ–‡), 'back' (å¾Œç½®)
            - ç« ç¯€ç·¨è™Ÿ: æ•¸å­—æˆ– None
        """
        import re

        # å¦‚æœå·²ç¶“æœ‰æ’åºè™Ÿï¼Œç›´æ¥ä½¿ç”¨
        if order_num is not None:
            return ('main', order_num)

        # å‰ç½®å…§å®¹çš„é—œéµå­—åŠå…¶å„ªå…ˆé †åº
        front_keywords = {
            '__no_chapter__': 0,  # å°é¢
            'å°é¢': 0,
            'cover': 0,
            'æ¨è–¦åº': 1,
            'æ¨è–¦': 1,
            'recommendation': 1,
            'åº': 2,
            'preface': 2,
            'å‰è¨€': 3,
            'foreword': 3,
            'introduction': 3,
            'å°è®€': 4,
            'ç›®éŒ„': 5,
            'contents': 5,
            'table of contents': 5,
            'ç›®æ¬¡': 5,
        }

        # å¾Œç½®å…§å®¹çš„é—œéµå­—
        back_keywords = [
            'é™„éŒ„', 'appendix', 'åƒè€ƒæ–‡ç»', 'references',
            'ç‰ˆæ¬Š', 'copyright', 'è‡´è¬', 'acknowledgment',
            'ä½œè€…', 'author', 'é—œæ–¼ä½œè€…', 'about the author',
            'å¾Œè¨˜', 'epilogue', 'afterword'
        ]

        chapter_lower = chapter_name.lower().strip()

        # æª¢æŸ¥æ˜¯å¦ç‚ºå‰ç½®å…§å®¹
        for keyword, priority in front_keywords.items():
            if keyword in chapter_lower:
                return ('front', priority)

        # æª¢æŸ¥æ˜¯å¦ç‚ºå¾Œç½®å…§å®¹
        for keyword in back_keywords:
            if keyword in chapter_lower:
                return ('back', 0)

        # å˜—è©¦æå–ç« ç¯€ç·¨è™Ÿï¼ˆæ­£æ–‡ï¼‰
        # æ¨¡å¼ 1: Chapter 1, Chapter 2, CHAPTER 1, etc.
        match = re.search(r'chapter\s+(\d+)', chapter_lower)
        if match:
            return ('main', int(match.group(1)))

        # æ¨¡å¼ 2: ç¬¬ä¸€ç« , ç¬¬äºŒç« , ç¬¬1ç« , ç¬¬2ç« 
        match = re.search(r'ç¬¬\s*([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+)\s*ç« ', chapter_name)
        if match:
            num_str = match.group(1)
            # è½‰æ›ä¸­æ–‡æ•¸å­—ç‚ºé˜¿æ‹‰ä¼¯æ•¸å­—
            chinese_nums = {'ä¸€': 1, 'äºŒ': 2, 'ä¸‰': 3, 'å››': 4, 'äº”': 5,
                            'å…­': 6, 'ä¸ƒ': 7, 'å…«': 8, 'ä¹': 9, 'å': 10,
                            'åä¸€': 11, 'åäºŒ': 12, 'åä¸‰': 13, 'åå››': 14, 'åäº”': 15,
                            'åå…­': 16, 'åä¸ƒ': 17, 'åå…«': 18, 'åä¹': 19, 'äºŒå': 20}
            if num_str in chinese_nums:
                return ('main', chinese_nums[num_str])
            elif num_str.isdigit():
                return ('main', int(num_str))

        # æ¨¡å¼ 3: 1. æ¨™é¡Œ, 2. æ¨™é¡Œ
        match = re.search(r'^(\d+)[\.ã€]\s*', chapter_name)
        if match:
            return ('main', int(match.group(1)))

        # æ¨¡å¼ 4: Chapter I, Chapter II (ç¾…é¦¬æ•¸å­—)
        match = re.search(r'chapter\s+([ivxlcdm]+)', chapter_lower)
        if match:
            roman = match.group(1).upper()
            # ç°¡å–®çš„ç¾…é¦¬æ•¸å­—è½‰æ›
            roman_values = {'I': 1, 'II': 2, 'III': 3, 'IV': 4, 'V': 5,
                            'VI': 6, 'VII': 7, 'VIII': 8, 'IX': 9, 'X': 10}
            if roman in roman_values:
                return ('main', roman_values[roman])

        # å¦‚æœç„¡æ³•è­˜åˆ¥ï¼Œè¦–ç‚ºå‰ç½®å…§å®¹ï¼Œæ”¾åœ¨æœ€å¾Œ
        return ('front', 999)

    def sort_chapters(self, chapter_order: list, chapters: dict) -> list:
        """
        å°ç« ç¯€é€²è¡Œæ™ºèƒ½æ’åº

        Args:
            chapter_order: åŸå§‹ç« ç¯€é †åºåˆ—è¡¨
            chapters: ç« ç¯€è³‡æ–™å­—å…¸

        Returns:
            æ’åºå¾Œçš„ç« ç¯€åˆ—è¡¨
        """
        # ç‚ºæ¯å€‹ç« ç¯€æå–æ’åºè³‡è¨Š
        chapter_info = []
        for chapter_name in chapter_order:
            # å¾ chapters å­—å…¸ä¸­ç²å–ç« ç¯€çš„ order_num
            chapter_data = chapters.get(chapter_name, {})
            order_num = chapter_data.get('order_num')

            chapter_type, chapter_num = self.extract_chapter_number(chapter_name, order_num)
            chapter_info.append((chapter_name, chapter_type, chapter_num))

        # æ’åºè¦å‰‡ï¼š
        # 1. å…ˆæŒ‰é¡å‹æ’åºï¼šfront < main < back
        # 2. åŒé¡å‹å…§æŒ‰ç·¨è™Ÿæ’åº
        type_order = {'front': 0, 'main': 1, 'back': 2}

        def sort_key(item):
            name, ch_type, ch_num = item
            type_priority = type_order[ch_type]
            num_priority = ch_num if ch_num is not None else 999
            return (type_priority, num_priority)

        sorted_info = sorted(chapter_info, key=sort_key)

        # è¿”å›æ’åºå¾Œçš„ç« ç¯€åç¨±åˆ—è¡¨
        return [name for name, _, _ in sorted_info]

    async def download_images_for_chapter(self, chapter_data: Dict[str, any], page_number: int, base_url: str = None):
        """
        ç‚ºç« ç¯€ä¸‹è¼‰æ‰€æœ‰åœ–ç‰‡ï¼ˆåŒ…å« figure, container ä¸­çš„åœ–ç‰‡ï¼‰

        Args:
            chapter_data: ç« ç¯€è³‡æ–™å­—å…¸
            page_number: é ç¢¼ï¼ˆç”¨æ–¼ç”Ÿæˆæª”æ¡ˆåï¼‰
            base_url: åŸºç¤ URL
        """
        # ä¸‹è¼‰ç¨ç«‹åœ–ç‰‡
        for image in chapter_data['images']:
            url = image['src']
            local_path = await self.download_image(url, page_number, base_url)
            image['local_path'] = local_path

        # ä¸‹è¼‰ figure ä¸­çš„åœ–ç‰‡
        for image in chapter_data.get('figure_images', []):
            url = image['src']
            local_path = await self.download_image(url, page_number, base_url)
            image['local_path'] = local_path
        
        # ä¸‹è¼‰ content_items ä¸­çš„åœ–ç‰‡ï¼ˆä¾†è‡ª div.containerï¼‰
        for item in chapter_data.get('content_items', []):
            if item.get('type') in ['image', 'figure']:
                img_src = item.get('image_src')
                if img_src:
                    # æª¢æŸ¥æ˜¯å¦å·²åœ¨ images æˆ– figure_images ä¸­
                    already_downloaded = False
                    for img in chapter_data['images']:
                        if img['src'] == img_src:
                            already_downloaded = True
                            break
                    if not already_downloaded:
                        for img in chapter_data.get('figure_images', []):
                            if img['src'] == img_src:
                                already_downloaded = True
                                break
                    
                    # å¦‚æœé‚„æ²’ä¸‹è¼‰ï¼Œæ·»åŠ åˆ° images åˆ—è¡¨ä¸¦ä¸‹è¼‰
                    if not already_downloaded:
                        local_path = await self.download_image(img_src, page_number, base_url)
                        chapter_data['images'].append({
                            'src': img_src,
                            'alt': item.get('image_alt', 'åœ–ç‰‡'),
                            'local_path': local_path
                        })

    def _generate_anchor_id(self, chapter_name: str) -> str:
        """
        å¾ç« ç¯€åç¨±ç”Ÿæˆ Markdown éŒ¨é» ID

        Args:
            chapter_name: ç« ç¯€åç¨±

        Returns:
            éŒ¨é» ID
        """
        import re
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œä¿ç•™ä¸­è‹±æ–‡æ•¸å­—
        anchor = re.sub(r'[^\w\s\-]', '', chapter_name)
        # æ›¿æ›ç©ºæ ¼ç‚ºé€£å­—ç¬¦
        anchor = re.sub(r'\s+', '-', anchor)
        return anchor.lower()

    async def convert_chapter_to_markdown(self, chapter_data: Dict[str, any], chapter_map: dict = None, toc_anchor: str = None, is_toc_chapter: bool = False) -> str:
        """
        å°‡ç« ç¯€è³‡æ–™è½‰æ›ç‚º Markdown æ ¼å¼

        Args:
            chapter_data: ç« ç¯€è³‡æ–™å­—å…¸
            chapter_map: ç« ç¯€åç¨±åˆ°éŒ¨é» ID çš„æ˜ å°„å­—å…¸ï¼ˆç”¨æ–¼ç›®éŒ„äº¤å‰å¼•ç”¨ï¼‰
            toc_anchor: ç›®éŒ„çš„éŒ¨é» IDï¼ˆç”¨æ–¼"å›åˆ°ç›®éŒ„"éˆæ¥ï¼‰
            is_toc_chapter: æ˜¯å¦ç‚ºç›®éŒ„ç« ç¯€

        Returns:
            Markdown æ ¼å¼çš„æ–‡å­—
        """
        markdown_lines = []

        # å¦‚æœæ˜¯ç›®éŒ„é ï¼Œç‰¹æ®Šè™•ç†
        if chapter_data.get('is_toc') and chapter_data.get('toc_links'):
            markdown_lines.append("\n## ç›®éŒ„\n\n")

            for toc_item in chapter_data['toc_links']:
                title = toc_item['title']

                # æŸ¥æ‰¾å°æ‡‰çš„ç« ç¯€éŒ¨é»
                if chapter_map:
                    # å˜—è©¦åœ¨ç« ç¯€æ˜ å°„ä¸­æ‰¾åˆ°åŒ¹é…çš„ç« ç¯€
                    anchor = None
                    for ch_name, ch_anchor in chapter_map.items():
                        # ç°¡å–®çš„æ¨™é¡ŒåŒ¹é…
                        if title in ch_name or ch_name in title:
                            anchor = ch_anchor
                            break

                    if anchor:
                        # ç”Ÿæˆå…§éƒ¨éˆæ¥
                        markdown_lines.append(f"- [{title}](#{anchor})\n")
                    else:
                        # æ²’æœ‰æ‰¾åˆ°å°æ‡‰ç« ç¯€ï¼Œåªé¡¯ç¤ºæ–‡æœ¬
                        markdown_lines.append(f"- {title}\n")
                else:
                    markdown_lines.append(f"- {title}\n")

            markdown_lines.append("\n")
            return ''.join(markdown_lines)

        # è™•ç†æœ‰åºå…§å®¹ï¼ˆåŒ…å« figure, image, caption, footnoteï¼‰
        for item in chapter_data['content_items']:
            item_type = item['type']
            content = item.get('content', '')

            if item_type == 'h1':
                markdown_lines.append(f"\n## {content}\n")
            elif item_type == 'h2':
                markdown_lines.append(f"\n### {content}\n")
            elif item_type == 'h3':
                markdown_lines.append(f"\n#### {content}\n")
            elif item_type == 'h4':
                markdown_lines.append(f"\n##### {content}\n")
            elif item_type == 'h5':
                markdown_lines.append(f"\n###### {content}\n")
            elif item_type == 'h6':
                markdown_lines.append(f"\n###### {content}\n")
            elif item_type == 'p':
                markdown_lines.append(f"{content}\n")
            elif item_type == 'image':
                # è™•ç†ç¨ç«‹åœ–ç‰‡ï¼ˆä¾†è‡ª div.containerï¼‰
                img_src = item.get('image_src', '')
                img_alt = item.get('image_alt', 'åœ–ç‰‡')

                # ä½¿ç”¨æœ¬åœ°è·¯å¾‘ï¼ˆå¦‚æœå·²ä¸‹è¼‰ï¼‰
                img_path = img_src
                for img in chapter_data.get('images', []):
                    if img['src'] == img_src:
                        img_path = img.get('local_path', img_src)
                        break

                markdown_lines.append(f"\n![{img_alt}]({img_path})\n")
            elif item_type == 'caption':
                # è™•ç†åœ–ç‰‡èªªæ˜æ–‡å­—ï¼ˆä¾†è‡ª div.containerï¼‰
                markdown_lines.append(f"\n*{content}*\n\n")
            elif item_type == 'figure':
                # è™•ç† figureï¼ˆåœ–ç‰‡ + èªªæ˜ï¼‰
                img_src = item.get('image_src', '')
                img_alt = item.get('image_alt', 'åœ–ç‰‡')

                # ä½¿ç”¨æœ¬åœ°è·¯å¾‘ï¼ˆå¦‚æœå·²ä¸‹è¼‰ï¼‰
                # æ³¨æ„ï¼šé€™è£¡éœ€è¦å¾ images åˆ—è¡¨ä¸­æŸ¥æ‰¾å°æ‡‰çš„æœ¬åœ°è·¯å¾‘
                img_path = img_src
                for img in chapter_data.get('figure_images', []):
                    if img['src'] == img_src:
                        img_path = img.get('local_path', img_src)
                        break

                markdown_lines.append(f"\n![{img_alt}]({img_path})\n\n")

        # è™•ç†ç¨ç«‹åœ–ç‰‡ï¼ˆä¸åœ¨ figure å’Œ container å…§çš„ï¼‰
        # æ”¶é›† content_items ä¸­å·²ç¶“è¼¸å‡ºçš„åœ–ç‰‡ URLï¼Œé¿å…é‡è¤‡
        output_image_srcs = set()
        for item in chapter_data['content_items']:
            if item.get('type') in ['image', 'figure']:
                img_src = item.get('image_src')
                if img_src:
                    output_image_srcs.add(img_src)
        
        # åªè¼¸å‡ºæœªåœ¨ content_items ä¸­å‡ºç¾çš„åœ–ç‰‡
        remaining_images = [img for img in chapter_data['images'] if img['src'] not in output_image_srcs]
        
        if remaining_images:
            markdown_lines.append("\n")
            for image in remaining_images:
                # å„ªå…ˆä½¿ç”¨æœ¬åœ°è·¯å¾‘
                img_path = image.get('local_path', image['src'])
                alt_text = image.get('alt', 'åœ–ç‰‡')
                markdown_lines.append(f"![{alt_text}]({img_path})\n")

        # è™•ç†è¨»é‡‹
        if chapter_data['footnotes']:
            markdown_lines.append("\n---\n\n**è¨»é‡‹ï¼š**\n\n")
            for footnote in chapter_data['footnotes']:
                markdown_lines.append(f"{footnote}\n\n")

        # åœ¨ç« ç¯€æœ«å°¾æ·»åŠ "å›åˆ°ç›®éŒ„"éˆæ¥ï¼ˆé™¤äº†ç›®éŒ„é æœ¬èº«ï¼‰
        # if not is_toc_chapter and toc_anchor:
        #     markdown_lines.append("\n---\n\n")
        #     markdown_lines.append(f"[ğŸ“š å›åˆ°ç›®éŒ„](#{toc_anchor})\n")

        return ''.join(markdown_lines)

    def convert_to_markdown(self, content: Dict[str, any], page_number: int = 0) -> str:
        """
        å°‡å…§å®¹è½‰æ›ç‚º Markdown æ ¼å¼

        Args:
            content: åŒ…å«æ¨™é¡Œã€æ®µè½å’Œåœ–ç‰‡çš„å­—å…¸
            page_number: é ç¢¼ï¼ˆç”¨æ–¼åœ–ç‰‡è·¯å¾‘ï¼‰

        Returns:
            Markdown æ ¼å¼çš„æ–‡å­—
        """
        markdown = []

        # è½‰æ›æ¨™é¡Œï¼ˆh1 -> ##, h2 -> ###, h3 -> ####, ä»¥æ­¤é¡æ¨ï¼‰
        for heading in content['headings']:
            level = int(heading['level'][1])  # h1 -> 1, h2 -> 2, h3 -> 3
            # h1 å°æ‡‰åˆ° ##ï¼ˆ2å€‹#ï¼‰ï¼Œh2 å°æ‡‰åˆ° ###ï¼ˆ3å€‹#ï¼‰
            prefix = '#' * (level + 1)
            markdown.append(f"{prefix} {heading['text']}\n")

        # è½‰æ›æ®µè½ï¼ˆå·²åŒ…å«ç²—é«”å’Œæ–œé«”ï¼‰
        for paragraph in content['paragraphs']:
            markdown.append(f"{paragraph}\n")

        # è½‰æ›åœ–ç‰‡ï¼ˆä½¿ç”¨æœ¬åœ°è·¯å¾‘æˆ– URLï¼‰
        for image in content['images']:
            alt_text = image['alt'] or 'åœ–ç‰‡'
            img_path = image.get('local_path', image['src'])  # å„ªå…ˆä½¿ç”¨æœ¬åœ°è·¯å¾‘
            markdown.append(f"![{alt_text}]({img_path})\n")

        return '\n'.join(markdown)

    async def get_reading_progress(self, page: Page) -> dict:
        """
        ç²å–é–±è®€é€²åº¦ä¿¡æ¯

        Args:
            page: Playwright é é¢ç‰©ä»¶

        Returns:
            åŒ…å«é€²åº¦ä¿¡æ¯çš„å­—å…¸ {'total_percent': 100, 'chapter_current': 4, 'chapter_total': 4}
        """
        try:
            # å®šä½é€²åº¦å®¹å™¨
            progress_container = page.locator('#page-info-container')

            # ç­‰å¾…å…ƒç´ å‡ºç¾
            await progress_container.wait_for(state="visible", timeout=5000)

            # ç²å–æ–‡å­—å…§å®¹
            progress_text = await progress_container.text_content()

            # è§£æé€²åº¦æ–‡å­—
            # æ ¼å¼ï¼šå…¨æ–‡ 10%ï¼æœ¬ç« ç¬¬ 1 é  / 4 é 
            progress_info = {
                'total_percent': 0,
                'chapter_current': 0,
                'chapter_total': 0,
                'text': progress_text.strip()
            }

            # æå–å…¨æ–‡ç™¾åˆ†æ¯”
            total_match = re.search(r'å…¨æ–‡\s*(\d+)%', progress_text)
            if total_match:
                progress_info['total_percent'] = int(total_match.group(1))

            # æå–æœ¬ç« é æ•¸
            chapter_match = re.search(r'æœ¬ç« ç¬¬?\s*(\d+)\s*é \s*/\s*(\d+)\s*é ', progress_text)
            if chapter_match:
                progress_info['chapter_current'] = int(chapter_match.group(1))
                progress_info['chapter_total'] = int(chapter_match.group(2))

            return progress_info

        except Exception as e:
            logger.info(f"      âš ï¸  ç„¡æ³•ç²å–é–±è®€é€²åº¦: {e}")
            return {
                'total_percent': 0,
                'chapter_current': 0,
                'chapter_total': 0,
                'text': ''
            }

    async def is_last_page(self, page: Page) -> bool:
        """
        æª¢æŸ¥æ˜¯å¦ç‚ºæœ€å¾Œä¸€é 

        Args:
            page: Playwright é é¢ç‰©ä»¶

        Returns:
            æ˜¯å¦ç‚ºæœ€å¾Œä¸€é 
        """
        progress = await self.get_reading_progress(page)

        # åˆ¤æ–·æ¢ä»¶ï¼šå…¨æ–‡ 100% ä¸”æœ¬ç« åˆ°æœ€å¾Œä¸€é 
        is_last = (
                progress['total_percent'] == 100 and
                progress['chapter_current'] > 0 and
                progress['chapter_current'] == progress['chapter_total']
        )

        return is_last

    async def turn_page(self, page: Page) -> bool:
        """
        ç¿»åˆ°ä¸‹ä¸€é ï¼ˆä½¿ç”¨é…ç½®çš„æŒ‰éµï¼‰

        Args:
            page: Playwright é é¢ç‰©ä»¶

        Returns:
            æ˜¯å¦æˆåŠŸç¿»é 
        """
        try:
            # æŒ‰ä¸‹é…ç½®çš„ç¿»é æŒ‰éµ
            await page.keyboard.press(self.page_turn_key)

            # ç­‰å¾…é é¢è¼‰å…¥
            await asyncio.sleep(0.1)

            return True

        except Exception as e:
            logger.warning(f"âš ï¸  ç¿»é æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return False

    async def download_images_for_content(self, content: Dict[str, any], page_number: int, base_url: str = None):
        """
        ä¸‹è¼‰å…§å®¹ä¸­çš„æ‰€æœ‰åœ–ç‰‡

        Args:
            content: åŒ…å«åœ–ç‰‡åˆ—è¡¨çš„å…§å®¹å­—å…¸
            page_number: é ç¢¼
            base_url: åŸºç¤ URLï¼ˆç”¨æ–¼è§£æç›¸å°è·¯å¾‘ï¼‰
        """
        if not self.download_images or not content['images']:
            return

        for image in content['images']:
            url = image['src']

            # ä¸‹è¼‰åœ–ç‰‡ä¸¦æ›´æ–°ç‚ºæœ¬åœ°è·¯å¾‘
            local_path = await self.download_image(url, page_number, base_url)
            image['local_path'] = local_path

    async def scrape_canvas_from_iframe(self, iframe: FrameLocator, page_number: int) -> list:
        """
        å¾å–®å€‹ iframe ä¸­æŠ“å–æ‰€æœ‰ Canvas åœ–ç‰‡ï¼ˆå¸¶ MD5 å»é‡ï¼‰

        Args:
            iframe: iframe locator
            page_number: é ç¢¼

        Returns:
            Canvas åœ–ç‰‡è³‡è¨Šåˆ—è¡¨
        """
        canvas_images = []
        
        try:
            body = iframe.locator('body')
            
            # æ‰¾åˆ°æ‰€æœ‰ Canvas å…ƒç´ 
            canvas_elements = body.locator('canvas')
            canvas_count = await canvas_elements.count()
            
            if canvas_count == 0:
                return canvas_images
            
            logger.info(f"         ğŸ¨ æ‰¾åˆ° {canvas_count} å€‹ Canvas å…ƒç´ ")
            
            for i in range(canvas_count):
                canvas = canvas_elements.nth(i)
                
                try:
                    # ç­‰å¾… Canvas æ¸²æŸ“å®Œæˆ
                    canvas_ready = False
                    for attempt in range(6):
                        has_content = await canvas.evaluate('''
                            canvas => {
                                try {
                                    const ctx = canvas.getContext('2d');
                                    if (!ctx) return false;
                                    
                                    const imageData = ctx.getImageData(0, 0, canvas.width, canvas.height);
                                    const data = imageData.data;
                                    
                                    for (let i = 3; i < data.length; i += 4) {
                                        if (data[i] > 0) return true;
                                    }
                                    return false;
                                } catch (e) {
                                    return false;
                                }
                            }
                        ''')
                        
                        if has_content:
                            canvas_ready = True
                            if attempt > 0:
                                logger.info(f"         âœ“ Canvas[{i}] å·²æ¸²æŸ“å®Œæˆï¼ˆå˜—è©¦ {attempt + 1} æ¬¡ï¼‰")
                            break
                        
                        if attempt < 5:
                            await asyncio.sleep(0.2)
                    
                    if not canvas_ready:
                        logger.info(f"         âš ï¸  Canvas[{i}] å¯èƒ½ç‚ºç©ºæˆ–æœªæ¸²æŸ“å®Œæˆï¼Œè·³é")
                        continue
                    
                    # è½‰æ›ç‚º data URL
                    data_url = await canvas.evaluate('''
                        canvas => {
                            try {
                                return canvas.toDataURL('image/png');
                            } catch (e) {
                                return null;
                            }
                        }
                    ''')
                    
                    if not data_url or not data_url.startswith('data:image'):
                        logger.info(f"         âš ï¸  Canvas[{i}] è½‰æ›å¤±æ•—")
                        continue
                    
                    # æª¢æŸ¥å¤§å°
                    data_size = len(data_url)
                    if data_size <= 5000:
                        logger.info(f"         âš ï¸  Canvas[{i}] åœ–ç‰‡éå° ({data_size} bytes)ï¼Œè·³é")
                        continue
                    
                    # è¨ˆç®— MD5 hash ç”¨æ–¼å»é‡
                    canvas_hash = hashlib.md5(data_url.encode()).hexdigest()
                    
                    # æª¢æŸ¥æ˜¯å¦é‡è¤‡
                    if canvas_hash in self.canvas_hashes:
                        logger.info(f"         ğŸ”„ Canvas[{i}] é‡è¤‡ï¼ˆMD5: {canvas_hash[:8]}...ï¼‰ï¼Œå·²è·³é")
                        continue
                    
                    # è¨˜éŒ„ hash
                    self.canvas_hashes.add(canvas_hash)
                    
                    # ä¿å­˜åœ–ç‰‡
                    import base64
                    match = re.match(r'data:image/(\w+);base64,(.+)', data_url)
                    if match:
                        img_format = match.group(1)
                        img_data = match.group(2)
                        
                        # ä½¿ç”¨ MD5 hash ä½œç‚ºæª”æ¡ˆåçš„ä¸€éƒ¨åˆ†ï¼ˆä¿è­‰å”¯ä¸€æ€§ï¼‰
                        filename = f"page_{page_number:04d}_canvas_{canvas_hash[:12]}.{img_format}"
                        local_path_full = self.images_dir / filename
                        
                        # è§£ç¢¼ä¸¦ä¿å­˜
                        with open(local_path_full, 'wb') as f:
                            f.write(base64.b64decode(img_data))
                        
                        relative_path = f"images/book_{self.book_id}/{filename}"
                        
                        canvas_images.append({
                            'page': page_number,
                            'canvas_index': i,
                            'path': relative_path,
                            'size': data_size,
                            'hash': canvas_hash
                        })
                        
                        logger.info(f"         âœ… Canvas[{i}] å·²ä¿å­˜: {filename} ({data_size / 1024:.1f} KB, MD5: {canvas_hash[:8]}...)")
                    
                except Exception as e:
                    logger.info(f"         âš ï¸  Canvas[{i}] è™•ç†å¤±æ•—: {e}")
                    continue
        
        except Exception as e:
            logger.info(f"         âš ï¸  æƒæ iframe Canvas å¤±æ•—: {e}")
        
        return canvas_images

    async def scrape_image_only_book(self, reading_page: Page) -> str:
        """
        çˆ¬å–ç´”åœ–ç‰‡æ›¸ç±ï¼ˆæ‰€æœ‰é é¢éƒ½æ˜¯ Canvasï¼‰

        Args:
            reading_page: é–±è®€é é¢çš„ Page ç‰©ä»¶

        Returns:
            å®Œæ•´çš„ Markdown å…§å®¹
        """
        logger.info("\n" + "=" * 60)
        logger.info("ğŸ“š é–‹å§‹çˆ¬å–ç´”åœ–ç‰‡æ›¸ç±ï¼ˆCanvas Only æ¨¡å¼ï¼‰")
        logger.info("=" * 60)
        
        # å»ºç«‹åœ–ç‰‡ç›®éŒ„
        self.images_dir = Path("downloads") / "images" / f"book_{self.book_id}"
        self.images_dir.mkdir(parents=True, exist_ok=True)
        logger.info(f"ğŸ“ åœ–ç‰‡å°‡ä¿å­˜åˆ°: {self.images_dir}")

        await asyncio.sleep(0.5)

        # è™•ç†é–±è®€é€²åº¦å½ˆçª—ï¼ˆå¦‚æœæœ‰ï¼‰
        await self.handle_reading_progress_popup(reading_page)

        await asyncio.sleep(0.5)

        # é»æ“Šã€Œæˆ‘çŸ¥é“äº†ã€æŒ‰éˆ•
        await self.click_accept_button(reading_page)

        # å„²å­˜æ‰€æœ‰ Canvas åœ–ç‰‡
        all_canvas_images = []
        page_number = 0
        consecutive_no_content = 0
        max_no_content = 10  # é€£çºŒ 10 é ç„¡å…§å®¹å°±åœæ­¢
        
        while page_number < self.max_pages and consecutive_no_content < max_no_content:
            page_number += 1
            
            # ç²å–é–±è®€é€²åº¦
            progress = await self.get_reading_progress(reading_page)
            logger.info(f"\nğŸ“– æ­£åœ¨æƒæç¬¬ {page_number} é ... [{progress['text']}] (é€²åº¦: {progress['total_percent']}%)")
            
            # ç²å–æ‰€æœ‰å¯è¦‹çš„ iframe
            visible_iframes = await self.get_all_visible_iframes(reading_page)
            
            found_canvas = False
            
            # å¾æ¯å€‹ iframe æŠ“å– Canvas
            for iframe_index, iframe in enumerate(visible_iframes):
                logger.info(f"      ğŸ“„ æ­£åœ¨æƒæ iframe[{iframe_index}]...")
                
                canvas_images = await self.scrape_canvas_from_iframe(iframe, page_number)
                
                if canvas_images:
                    all_canvas_images.extend(canvas_images)
                    found_canvas = True
                    logger.info(f"      âœ… iframe[{iframe_index}] æ‰¾åˆ° {len(canvas_images)} å¼µæ–°åœ–ç‰‡")
                else:
                    logger.info(f"      â„¹ï¸  iframe[{iframe_index}] ç„¡æ–° Canvas åœ–ç‰‡")
            
            # æ›´æ–°é€£çºŒç„¡å…§å®¹è¨ˆæ•¸
            if found_canvas:
                consecutive_no_content = 0
            else:
                consecutive_no_content += 1
                logger.info(f"   âš ï¸  æœ¬é ç„¡æ–°å…§å®¹ï¼ˆé€£çºŒ {consecutive_no_content}/{max_no_content}ï¼‰")
            
            # æª¢æŸ¥çµ‚æ­¢æ¢ä»¶
            # 1. æª¢æ¸¬ã€Œé–±è®€çµæŸã€æ¨™è¨˜
            try:
                reading_end = reading_page.locator('div.sc-1wqquil-3:has-text("é–±è®€çµæŸ")')
                if await reading_end.count() > 0:
                    logger.success("âœ… æª¢æ¸¬åˆ°ã€Œé–±è®€çµæŸã€æ¨™è¨˜ï¼Œåœæ­¢çˆ¬å–")
                    break
            except:
                pass
            
            # 2. æª¢æŸ¥æ˜¯å¦ç‚ºæœ€å¾Œä¸€é 
            if await self.is_last_page(reading_page):
                logger.success("âœ… å·²åˆ°é”æœ€å¾Œä¸€é ï¼ˆå…¨æ–‡ 100% ä¸”æœ¬ç« æœ€å¾Œä¸€é ï¼‰")
                break
            
            # 3. é€£çºŒç„¡æ–°å…§å®¹
            if consecutive_no_content >= max_no_content:
                logger.warning(f"âš ï¸  é€£çºŒ {max_no_content} é ç„¡æ–°å…§å®¹ï¼Œåœæ­¢çˆ¬å–")
                break
            
            # ç¿»é 
            logger.info(f"   â­ï¸  ç¿»åˆ°ä¸‹ä¸€é ...")
            success = await self.turn_page(reading_page)
            if not success:
                logger.info(f"   âš ï¸  ç¿»é å¤±æ•—")
                break
            
            await asyncio.sleep(0.1)
        
        logger.info("\n" + "=" * 60)
        logger.success(f"âœ… çˆ¬å–å®Œæˆï¼")
        logger.info(f"   - å…±æƒæ: {page_number} é ")
        logger.info(f"   - æ‰¾åˆ°åœ–ç‰‡: {len(all_canvas_images)} å¼µ")
        logger.info(f"   - å»é‡å¾Œ: {len(self.canvas_hashes)} å¼µå”¯ä¸€åœ–ç‰‡")
        logger.info("=" * 60)
        
        # ç”Ÿæˆ Markdown å…§å®¹
        markdown_lines = []
        
        for idx, img in enumerate(all_canvas_images, 1):
            markdown_lines.append(f"![ç¬¬ {img['page']} é ]({img['path']})\n")
        
        return '\n'.join(markdown_lines)

    def _get_item_preview(self, item: dict) -> str:
        """
        ç²å– content_item çš„é è¦½æ–‡å­—ï¼ˆè™•ç†ä¸åŒé¡å‹ï¼‰
        
        Args:
            item: content_item å­—å…¸
            
        Returns:
            é è¦½æ–‡å­—ï¼ˆæœ€å¤š 60 å­—ç¬¦ï¼‰
        """
        item_type = item.get('type', 'unknown')
        
        if item_type == 'image':
            # image é¡å‹ï¼šé¡¯ç¤ºåœ–ç‰‡ä¾†æº
            img_src = item.get('image_src', '')
            img_alt = item.get('image_alt', 'åœ–ç‰‡')
            return f"[åœ–ç‰‡] {img_alt} ({img_src[:40]}...)" if len(img_src) > 40 else f"[åœ–ç‰‡] {img_alt} ({img_src})"
        elif item_type == 'figure':
            # figure é¡å‹ï¼šé¡¯ç¤ºèªªæ˜æ–‡å­—å’Œåœ–ç‰‡ä¾†æº
            content = item.get('content', '')
            img_src = item.get('image_src', '')
            preview = content[:30] if len(content) > 30 else content
            return f"[åœ–è¡¨] {preview}... ({img_src[:20]}...)" if len(content) > 30 else f"[åœ–è¡¨] {preview} ({img_src[:20]}...)"
        elif item_type == 'caption':
            # caption é¡å‹ï¼šé¡¯ç¤ºèªªæ˜æ–‡å­—
            content = item.get('content', '')
            return f"[èªªæ˜] {content[:50]}..." if len(content) > 50 else f"[èªªæ˜] {content}"
        else:
            # å…¶ä»–é¡å‹ï¼ˆh1-h6, pï¼‰ï¼šé¡¯ç¤ºæ–‡å­—å…§å®¹
            content = item.get('content', '')
            return f"{content[:60]}..." if len(content) > 60 else content
    
    def _renumber_footnotes(self, chapters_list: list, starting_number: int = 1) -> int:
        """
        ç‚ºæ‰€æœ‰ç« ç¯€çš„ footnote é‡æ–°ç·¨è™Ÿï¼ˆé¿å…è·¨ç« ç¯€ç·¨è™Ÿè¡çªï¼‰
        
        Args:
            chapters_list: ç« ç¯€åˆ—è¡¨ [(chapter_data, content_hash), ...]
            starting_number: èµ·å§‹ç·¨è™Ÿ
            
        Returns:
            ä¸‹ä¸€å€‹å¯ç”¨çš„ footnote ç·¨è™Ÿ
        """
        current_number = starting_number
        
        for chapter_data, _ in chapters_list:
            # å»ºç«‹è©²ç« ç¯€çš„ footnote ç·¨è™Ÿæ˜ å°„è¡¨ (åŸç·¨è™Ÿ -> æ–°ç·¨è™Ÿ)
            footnote_map = {}
            
            # ç¬¬ä¸€æ­¥ï¼šå…ˆæ”¶é›†æ‰€æœ‰ footnote å®šç¾©ï¼Œå»ºç«‹æ˜ å°„è¡¨
            # åªæƒæå®šç¾©ï¼Œä¸æƒæå¼•ç”¨ï¼Œé¿å…é‡è¤‡è¨ˆæ•¸
            for item in chapter_data.get('content_items', []):
                if item.get('type') == 'p':
                    content = item.get('content', '')
                    # æª¢æŸ¥æ˜¯å¦ç‚º footnote å®šç¾©ï¼ˆä»¥ [^æ•¸å­—]: é–‹é ­ï¼‰
                    footnote_def_match = re.match(r'\[\^(\d+)\]:', content)
                    if footnote_def_match:
                        old_num = footnote_def_match.group(1)
                        if old_num not in footnote_map:
                            footnote_map[old_num] = str(current_number)
                            current_number += 1
            
            # ç¬¬äºŒæ­¥ï¼šæ›¿æ›æ‰€æœ‰ content_items ä¸­çš„ footnote å¼•ç”¨å’Œå®šç¾©ç·¨è™Ÿ
            for item in chapter_data.get('content_items', []):
                if item.get('type') in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'caption']:
                    content = item.get('content', '')
                    
                    # æ›¿æ›æ‰€æœ‰ footnote å¼•ç”¨å’Œå®šç¾©
                    # æ³¨æ„ï¼šå¿…é ˆæŒ‰ç…§å¾å¤§åˆ°å°çš„é †åºæ›¿æ›ï¼Œé¿å…å­ä¸²æ›¿æ›å•é¡Œ
                    # ä¾‹å¦‚ï¼šå…ˆæ›¿æ› [^10] å†æ›¿æ› [^1]ï¼Œå¦å‰‡ [^10] æœƒè®Šæˆ [^æ–°1]0
                    sorted_old_nums = sorted(footnote_map.keys(), key=lambda x: int(x), reverse=True)
                    
                    for old_num in sorted_old_nums:
                        new_num = footnote_map[old_num]
                        # æ›¿æ›å¼•ç”¨ï¼š[^1] -> [^æ–°ç·¨è™Ÿ]
                        content = re.sub(rf'\[\^{old_num}\](?!:)', f'[^{new_num}]', content)
                        # æ›¿æ›å®šç¾©ï¼š[^1]: -> [^æ–°ç·¨è™Ÿ]:
                        content = re.sub(rf'\[\^{old_num}\]:', f'[^{new_num}]:', content)
                    
                    item['content'] = content
        
        return current_number

    def _generate_chapter_hash(self, chapter_data: Dict[str, any]) -> str:
        """
        ç‚ºç« ç¯€å…§å®¹ç”Ÿæˆå”¯ä¸€çš„å“ˆå¸Œå€¼ï¼ˆåŸºæ–¼æ–‡å­—å…§å®¹å’Œåœ–ç‰‡ï¼‰

        Args:
            chapter_data: ç« ç¯€è³‡æ–™å­—å…¸

        Returns:
            MD5 å“ˆå¸Œå€¼
        """
        # æ”¶é›†æ‰€æœ‰æ–‡å­—å…§å®¹å’Œåœ–ç‰‡ä¿¡æ¯
        content_parts = []
        
        for item in chapter_data.get('content_items', []):
            item_type = item.get('type', '')
            
            if item_type == 'image':
                # image é¡å‹ï¼šä½¿ç”¨åœ–ç‰‡ä¾†æº
                content_parts.append(f"[IMAGE:{item.get('image_src', '')}]")
            elif item_type == 'figure':
                # figure é¡å‹ï¼šä½¿ç”¨èªªæ˜æ–‡å­— + åœ–ç‰‡ä¾†æº
                content_parts.append(f"[FIGURE:{item.get('content', '')}:{item.get('image_src', '')}]")
            else:
                # å…¶ä»–é¡å‹ï¼šä½¿ç”¨æ–‡å­—å…§å®¹
                content_parts.append(item.get('content', ''))
        
        # æ”¶é›†æ‰€æœ‰ç¨ç«‹åœ–ç‰‡ URL
        for img in chapter_data.get('images', []):
            content_parts.append(f"[IMG:{img.get('src', '')}]")
        
        # æ”¶é›†æ‰€æœ‰ figure åœ–ç‰‡ URL
        for img in chapter_data.get('figure_images', []):
            content_parts.append(f"[FIG:{img.get('src', '')}]")
        
        # çµ„åˆæˆå”¯ä¸€å­—ç¬¦ä¸²
        unique_string = '|||'.join(content_parts)
        
        # ç”Ÿæˆ MD5 å“ˆå¸Œ
        return hashlib.md5(unique_string.encode('utf-8')).hexdigest()

    async def scrape_entire_book(self, reading_page: Page) -> str:
        """
        çˆ¬å–æ•´æœ¬æ›¸çš„å…§å®¹ï¼ˆæŒ‰ iframe å‡ºç¾é †åºï¼Œä½¿ç”¨å…§å®¹å“ˆå¸Œå»é‡ï¼‰

        Args:
            reading_page: é–±è®€é é¢çš„ Page ç‰©ä»¶

        Returns:
            å®Œæ•´çš„ Markdown å…§å®¹
        """
        logger.info("\n" + "=" * 60)
        logger.info("ğŸ“š é–‹å§‹çˆ¬å–æ›¸ç±å…§å®¹ï¼ˆæŒ‰ iframe é †åºï¼‰")
        logger.info("=" * 60)

        # å¦‚æœéœ€è¦ä¸‹è¼‰åœ–ç‰‡ï¼Œå»ºç«‹åœ–ç‰‡ç›®éŒ„
        if self.download_images:
            self.images_dir = Path("downloads") / "images" / f"book_{self.book_id}"
            self.images_dir.mkdir(parents=True, exist_ok=True)
            logger.info(f"ğŸ“ åœ–ç‰‡å°‡ä¿å­˜åˆ°: {self.images_dir}")

        # ç­‰å¾…é é¢å®Œå…¨è¼‰å…¥
        await asyncio.sleep(0.5)

        # è™•ç†é–±è®€é€²åº¦å½ˆçª—ï¼ˆå¦‚æœæœ‰ï¼‰
        await self.handle_reading_progress_popup(reading_page)

        # ç­‰å¾…é é¢å®Œå…¨è¼‰å…¥
        await asyncio.sleep(0.5)

        # é»æ“Šã€Œæˆ‘çŸ¥é“äº†ã€æŒ‰éˆ•
        await self.click_accept_button(reading_page)

        # ä½¿ç”¨åˆ—è¡¨æŒ‰é †åºå­˜å„²ç« ç¯€ï¼ˆä¿æŒ iframe å‡ºç¾é †åºï¼‰
        chapters_list = []  # [(chapter_data, chapter_hash), ...]
        processed_hashes = set()  # å·²è™•ç†çš„å…§å®¹å“ˆå¸Œ
        toc_links = []  # TOC ç›®éŒ„éˆæ¥ï¼ˆç”¨æ–¼æ™ºèƒ½æ’åºï¼‰

        page_number = 0
        full_progress_count = 0  # è¨˜éŒ„é€£çºŒå‡ºç¾å…¨æ–‡ 100% çš„æ¬¡æ•¸

        # ç²å– base URLï¼ˆç”¨æ–¼åœ–ç‰‡ä¸‹è¼‰ï¼‰
        base_url = await self.get_base_url_from_iframe(reading_page)
        if base_url:
            logger.info(f"ğŸ“ Base URL: {base_url}")

        # ğŸ” å˜—è©¦å¾ç¬¬ä¸€é æå– TOCï¼ˆç›®éŒ„ï¼‰ä¿¡æ¯
        try:
            first_iframes = await self.get_all_visible_iframes(reading_page)
            for iframe in first_iframes:
                if await self.is_toc_page(iframe):
                    toc_links = await self.extract_toc_links(iframe)
                    if toc_links:
                        logger.success(f"âœ… å·²æå– TOC ç›®éŒ„ï¼ˆå…± {len(toc_links)} é …ï¼‰")
                        break
        except Exception as e:
            logger.warning(f"âš ï¸  æå– TOC å¤±æ•—: {e}")

        while page_number < self.max_pages:
            page_number += 1

            # ç²å–é–±è®€é€²åº¦
            progress = await self.get_reading_progress(reading_page)
            logger.info(f"\nğŸ“– æ­£åœ¨æƒæç¬¬ {page_number} é ... [{progress['text']}] (é€²åº¦: {progress['total_percent']}%)")

            # ç²å–æ‰€æœ‰å¯è¦‹çš„ iframeï¼ˆæŒ‰é †åºï¼‰
            visible_iframes = await self.get_all_visible_iframes(reading_page)

            found_new_content = False

            # æŒ‰ iframe[0], iframe[1], iframe[2]... çš„é †åºè™•ç†
            for iframe_index, iframe in enumerate(visible_iframes):
                logger.info(f"      ğŸ“„ æ­£åœ¨æŠ“å– iframe[{iframe_index}]...")

                # æŠ“å–ç« ç¯€è³‡æ–™ï¼ˆå‚³é TOC ç”¨æ–¼æ™ºèƒ½æ’åºï¼‰
                chapter_data = await self.scrape_chapter_from_iframe(iframe, base_url, toc_links)

                if not chapter_data:
                    logger.info(f"         âš ï¸  iframe[{iframe_index}] æ²’æœ‰å…§å®¹")
                    continue

                # ç”Ÿæˆå…§å®¹å“ˆå¸Œï¼ˆåŸºæ–¼æ–‡å­—+åœ–ç‰‡ï¼‰
                content_hash = self._generate_chapter_hash(chapter_data)

                # æª¢æŸ¥æ˜¯å¦ç‚ºæ–°å…§å®¹ï¼ˆç”¨å“ˆå¸Œåˆ¤æ–·ï¼Œä¸ç”¨ç« ç¯€åï¼‰
                if content_hash not in processed_hashes:
                    # æ–°å…§å®¹ï¼ŒåŠ å…¥åˆ—è¡¨
                    chapters_list.append((chapter_data, content_hash))
                    processed_hashes.add(content_hash)
                    found_new_content = True

                    chapter_name = chapter_data['name']
                    display_name = chapter_name if chapter_name != "__no_chapter__" else "ã€ç„¡ç« ç¯€åç¨±ã€‘"
                    logger.info(f"         âœ… æ–°å…§å®¹ (#{len(chapters_list)}): {display_name}")
                    logger.info(f"            å“ˆå¸Œ: {content_hash[:12]}...")

                    # DEBUG: é¡¯ç¤ºå…§å®¹é è¦½
                    if chapter_data['content_items']:
                        first_item = chapter_data['content_items'][0]
                        last_item = chapter_data['content_items'][-1]
                        
                        # ç²å–ç¬¬ä¸€é …é è¦½ï¼ˆè™•ç†ä¸åŒé¡å‹ï¼‰
                        first_preview = self._get_item_preview(first_item)
                        logger.debug(f"         ğŸ” ç¬¬ä¸€é … ({first_item['type']}): {first_preview}")
                        
                        # ç²å–æœ€å¾Œé …é è¦½ï¼ˆè™•ç†ä¸åŒé¡å‹ï¼‰
                        last_preview = self._get_item_preview(last_item)
                        logger.debug(f"         ğŸ” æœ€å¾Œé … ({last_item['type']}): {last_preview}")

                    total_images = len(chapter_data['images']) + len(chapter_data.get('figure_images', []))
                    logger.info(f"         ğŸ“Š çµ±è¨ˆ: {len(chapter_data['content_items'])} å€‹å…ƒç´ , {total_images} å¼µåœ–ç‰‡")

                    # ä¸‹è¼‰åœ–ç‰‡ï¼ˆåŒ…æ‹¬ figure ä¸­çš„åœ–ç‰‡ï¼‰
                    if self.download_images and total_images > 0:
                        await self.download_images_for_chapter(chapter_data, page_number, base_url)
                else:
                    logger.debug(f"         ğŸ”„ iframe[{iframe_index}] å…§å®¹é‡è¤‡ï¼ˆå“ˆå¸Œ: {content_hash[:12]}...ï¼‰")

            # å¦‚æœæ²’æœ‰æ‰¾åˆ°æ–°å…§å®¹ï¼Œåªæ˜¯æç¤ºï¼Œä¸ä½œç‚ºçµ‚æ­¢æ¢ä»¶
            if not found_new_content:
                logger.info(f"   â„¹ï¸  æœ¬é æ‰€æœ‰ iframe éƒ½æ˜¯å·²è™•ç†éçš„å…§å®¹")

            # æª¢æŸ¥æ˜¯å¦é¡¯ç¤º"é–±è®€çµæŸ"ï¼ˆå„ªå…ˆçµ‚æ­¢æ¢ä»¶ï¼‰
            try:
                reading_end = reading_page.locator('div.sc-1wqquil-3:has-text("é–±è®€çµæŸ")')
                if await reading_end.count() > 0:
                    logger.success("âœ… æª¢æ¸¬åˆ°ã€Œé–±è®€çµæŸã€æ¨™è¨˜ï¼Œåœæ­¢çˆ¬å–")
                    break
            except Exception as e:
                pass  # å¿½ç•¥éŒ¯èª¤ï¼Œç¹¼çºŒæª¢æŸ¥å…¶ä»–æ¢ä»¶

            # æª¢æŸ¥æ˜¯å¦ç‚ºæœ€å¾Œä¸€é ï¼ˆä¸»è¦çµ‚æ­¢æ¢ä»¶ï¼‰
            if await self.is_last_page(reading_page):
                logger.success("âœ… å·²åˆ°é”æœ€å¾Œä¸€é ï¼ˆå…¨æ–‡ 100% ä¸”æœ¬ç« æœ€å¾Œä¸€é ï¼‰")
                break

            # å®‰å…¨æ©Ÿåˆ¶ï¼šæª¢æ¸¬å…¨æ–‡ 100% çš„æƒ…æ³
            if progress['total_percent'] >= 100:
                full_progress_count += 1

                if not found_new_content:
                    # å¦‚æœå…¨æ–‡ 100% ä¸”æ²’æœ‰æ–°å…§å®¹
                    logger.info(f"   âš ï¸  å·²é”å…¨æ–‡ 100% ä¸”ç„¡æ–°å…§å®¹ï¼ˆç¬¬ {full_progress_count} æ¬¡ï¼‰")

                    if full_progress_count >= 5:
                        # é€£çºŒ 5 æ¬¡ 100% ä¸”ç„¡æ–°å…§å®¹ï¼Œæå‰çµ‚æ­¢
                        logger.info("   ğŸ›‘ é€£çºŒ 5 æ¬¡åµæ¸¬åˆ°å…¨æ–‡ 100% ä¸”ç„¡æ–°å…§å®¹ï¼Œåœæ­¢çˆ¬å–")
                        logger.info("   ğŸ’¡ æç¤ºï¼šé€™å¯èƒ½æ˜¯ç¶²ç«™é€²åº¦é¡¯ç¤ºéŒ¯èª¤ï¼ˆä¾‹å¦‚ï¼šå…¨æ–‡ 100%ï¼æœ¬ç« ç¬¬ 1 é  / 2 é ï¼‰")
                        break
                else:
                    # æœ‰æ–°å…§å®¹ï¼Œèªªæ˜é‚„æ²’çµæŸï¼Œåªæ˜¯é¡¯ç¤º 100%
                    logger.info(f"   â„¹ï¸  å·²é”å…¨æ–‡ 100% ä½†ç™¼ç¾æ–°å…§å®¹ï¼Œç¹¼çºŒçˆ¬å–...")
                    full_progress_count = 0

                if full_progress_count >= 10:
                    # ä¿éšªæ©Ÿåˆ¶ï¼šç„¡è«–å¦‚ä½•ï¼Œé€£çºŒ 10 æ¬¡ 100% å°±åœæ­¢
                    logger.info("   ğŸ›‘ é€£çºŒ 10 æ¬¡åµæ¸¬åˆ°å…¨æ–‡ 100%ï¼Œå¼·åˆ¶åœæ­¢çˆ¬å–")
                    break
            else:
                # é‡ç½®è¨ˆæ•¸å™¨
                full_progress_count = 0

            # æ ¹æ“šè¨­å®šé¸æ“‡ç¿»é ç­–ç•¥
            if self.smart_page_turn:
                # æ™ºèƒ½ç¿»é ï¼šæ ¹æ“šæœ¬ç« å‰©é¤˜é æ•¸æ±ºå®šç¿»å¤šå°‘æ¬¡ï¼ˆè€ƒæ…® turn_page å¯èƒ½ä¸€æ¬¡ç¿»2é ï¼‰
                remaining_pages = progress['chapter_total'] - progress['chapter_current']
                current_chapter_page = progress['chapter_current']

                if remaining_pages <= 0:
                    # ç« ç¯€çµæŸï¼Œåªç¿» 1 æ¬¡åˆ°ä¸‹ä¸€ç« 
                    turn_count = 1
                    logger.info(f"   â­ï¸  ç« ç¯€å·²çµæŸï¼Œç¿» 1 æ¬¡åˆ°ä¸‹ä¸€ç« ...")
                elif remaining_pages <= 5:
                    # æ¥è¿‘ç« ç¯€å°¾éƒ¨ï¼Œåªç¿» 1 æ¬¡ï¼ˆé¿å…è·³éå…§å®¹ï¼‰
                    turn_count = 1
                    logger.info(f"   â­ï¸  æœ¬ç« å‰©é¤˜ {remaining_pages} é ï¼Œè¬¹æ…ç¿» 1 æ¬¡ï¼ˆç•¶å‰ç¬¬ {current_chapter_page}/{progress['chapter_total']} é ï¼‰...")
                elif remaining_pages <= 10:
                    # ç« ç¯€ä¸­å¾Œæ®µï¼Œç¿» 2 æ¬¡
                    turn_count = 2
                    logger.info(f"   â­ï¸  æœ¬ç« å‰©é¤˜ {remaining_pages} é ï¼Œç¿» 2 æ¬¡...")
                elif remaining_pages > 15:
                    # ç« ç¯€å‰æ®µï¼Œå¿«é€Ÿç¿»åˆ°æ¥è¿‘æœ«å°¾ï¼ˆä¿ç•™æœ€å¾Œ 5 é æ…¢æ…¢ç¿»ï¼‰
                    # è¨ˆç®—éœ€è¦ç¿»å¹¾æ¬¡æ‰èƒ½åˆ°å‰©é¤˜ 5 é ï¼ˆå‡è¨­æ¯æ¬¡ç¿» 2 é ï¼‰
                    target_remaining = 5
                    pages_to_skip = remaining_pages - target_remaining
                    # ä¿å®ˆä¼°è¨ˆï¼šæ¯æ¬¡ç¿»é å¯èƒ½ç§»å‹• 1-2 é ï¼Œæˆ‘å€‘æŒ‰ 1.5 é è¨ˆç®—
                    calculated_turns = max(1, int(pages_to_skip / 1.5))
                    # é™åˆ¶æ¯æ¬¡æœ€å¤šç¿» 10 æ¬¡ï¼ˆé¿å…ä¸€æ¬¡è·³å¤ªå¤šï¼‰
                    turn_count = min(calculated_turns, 10)
                    logger.info(f"   ğŸš€ æœ¬ç« å‰©é¤˜ {remaining_pages} é ï¼Œå¿«é€Ÿç¿» {turn_count} æ¬¡ï¼ˆä¸Šé™: 10 æ¬¡ï¼‰...")
                else:
                    # ç« ç¯€ä¸­æ®µï¼ˆ11-15é ï¼‰ï¼Œç¿» 3 æ¬¡
                    turn_count = 3
                    logger.info(f"   â­ï¸  æœ¬ç« å‰©é¤˜ {remaining_pages} é ï¼Œç¿» {turn_count} æ¬¡...")
            else:
                # å›ºå®šç¿»é ï¼šæ¯æ¬¡ç¿»å›ºå®šæ¬¡æ•¸
                turn_count = self.pages_per_turn
                logger.info(f"   â­ï¸  ä½¿ç”¨å›ºå®šç¿»é ç­–ç•¥ï¼Œç¿» {turn_count} æ¬¡...")

            # åŸ·è¡Œç¿»é 
            for i in range(turn_count):
                if page_number + i >= self.max_pages:
                    break

                success = await self.turn_page(reading_page)
                if not success:
                    logger.warning(f"   âš ï¸  ç¬¬ {i+1} æ¬¡ç¿»é å¤±æ•—")
                    break

                # ç­‰å¾…é é¢åŠ è¼‰
                await asyncio.sleep(0.3)
                
                # åœ¨é—œéµä½ç½®ï¼ˆå‰©é¤˜5é ä»¥å…§ï¼‰æª¢æŸ¥å¯¦éš›é€²åº¦
                if self.smart_page_turn and i == 0 and remaining_pages <= 5:
                    new_progress = await self.get_reading_progress(reading_page)
                    actual_moved = new_progress['chapter_current'] - current_chapter_page
                    if actual_moved > 1:
                        logger.debug(f"      ğŸ’¡ æª¢æ¸¬åˆ°ç¿»é å¯¦éš›ç§»å‹•äº† {actual_moved} é ï¼ˆå¾ {current_chapter_page} â†’ {new_progress['chapter_current']}ï¼‰")
                        # å¦‚æœä¸€æ¬¡ç¿»äº†å¤šé ï¼Œå°±ä¸å†ç¹¼çºŒç¿»äº†
                        break

            page_number += (turn_count - 1)  # å¾ªç’°æœƒå† +1

        logger.info("\n" + "=" * 60)
        logger.success(f"âœ… çˆ¬å–å®Œæˆï¼å…±æ‰¾åˆ° {len(chapters_list)} å€‹ä¸é‡è¤‡çš„å…§å®¹å€å¡Š (æƒæ {page_number} é )")
        logger.info("=" * 60)

        # å…§å®¹å·²ç¶“æŒ‰ iframe é †åºå­˜å„²ï¼Œç„¡éœ€æ’åº
        logger.info("\n" + "=" * 60)
        logger.info("ğŸ“– å…§å®¹å·²æŒ‰ iframe å‡ºç¾é †åºæ’åˆ—ï¼ˆç„¡éœ€é‡æ–°æ’åºï¼‰")
        logger.info("=" * 60)

        # å»ºç«‹ç« ç¯€åç¨±åˆ°éŒ¨é» ID çš„æ˜ å°„
        chapter_map = {}
        toc_anchor = None  # ç›®éŒ„çš„éŒ¨é» ID

        # å…ˆæƒæä¸€éï¼Œå»ºç«‹éŒ¨é»æ˜ å°„
        for idx, (chapter_data, _) in enumerate(chapters_list):
            chapter_name = chapter_data['name']
            if chapter_name == "ç›®éŒ„":
                toc_anchor = "toc"
                chapter_map[chapter_name] = toc_anchor
            elif chapter_name != "__no_chapter__":
                # ç‚ºæ¯å€‹ç« ç¯€ç”Ÿæˆå”¯ä¸€éŒ¨é»ï¼ˆåŠ ä¸Šç´¢å¼•é¿å…é‡è¤‡ï¼‰
                anchor_id = f"{self._generate_anchor_id(chapter_name)}-{idx}"
                chapter_map[chapter_name] = anchor_id

        # é‡æ–°ç·¨è™Ÿæ‰€æœ‰ç« ç¯€çš„ footnoteï¼ˆé¿å…è·¨ç« ç¯€ç·¨è™Ÿè¡çªï¼‰
        logger.info("\nğŸ”¢ é‡æ–°ç·¨è™Ÿ footnote...")
        footnote_count = self._renumber_footnotes(chapters_list)
        if footnote_count > 1:
            logger.info(f"   âœ… å·²é‡æ–°ç·¨è™Ÿ {footnote_count - 1} å€‹ footnote")

        # æŒ‰é †åºè½‰æ›ç‚º Markdown
        all_markdown = []

        for idx, (chapter_data, content_hash) in enumerate(chapters_list, 1):
            chapter_name = chapter_data['name']
            display_name = chapter_name if chapter_name != "__no_chapter__" else "ã€ç„¡ç« ç¯€åç¨±ã€‘"
            logger.info(f"ğŸ“ ç¬¬ {idx} å€‹å€å¡Š: {display_name} (å“ˆå¸Œ: {content_hash[:12]}...)")

            # ç‚ºéç›®éŒ„ç« ç¯€æ·»åŠ éŒ¨é»
            chapter_markdown_parts = []

            if chapter_name in chapter_map:
                # æ·»åŠ éŒ¨é»
                anchor_id = chapter_map[chapter_name]
                chapter_markdown_parts.append(f'<a name="{anchor_id}"></a>\n\n')

            # è½‰æ›ç« ç¯€å…§å®¹ï¼ˆå‚³å…¥ chapter_map å’Œ toc_anchor ç”¨æ–¼äº¤å‰å¼•ç”¨ï¼‰
            chapter_content = await self.convert_chapter_to_markdown(
                chapter_data,
                chapter_map,
                toc_anchor=toc_anchor,
                is_toc_chapter=(chapter_name == "ç›®éŒ„")
            )
            chapter_markdown_parts.append(chapter_content)

            all_markdown.append(''.join(chapter_markdown_parts))

        return '\n\n'.join(all_markdown)

    async def run(self, headless: bool = False, slow_mo: int = 100, wait_time: int = 10) -> bool:
        """
        åŸ·è¡Œå®Œæ•´çš„å€Ÿé–±æµç¨‹ï¼ˆåŒ…å«çˆ¬èŸ²ï¼‰

        Args:
            headless: æ˜¯å¦ä½¿ç”¨ç„¡é ­æ¨¡å¼ï¼ˆä¸é¡¯ç¤ºç€è¦½å™¨è¦–çª—ï¼‰
            slow_mo: æ¸›æ…¢æ“ä½œé€Ÿåº¦ï¼ˆæ¯«ç§’ï¼‰ï¼Œä¾¿æ–¼è§€å¯Ÿ
            wait_time: æˆåŠŸå¾Œç­‰å¾…æ™‚é–“ï¼ˆç§’ï¼‰ï¼Œè®“ä½¿ç”¨è€…çœ‹åˆ°çµæœ

        Returns:
            åŸ·è¡Œæ˜¯å¦æˆåŠŸ
        """
        async with async_playwright() as p:
            # å•Ÿå‹•ç€è¦½å™¨
            logger.info(f"ğŸŒ æ­£åœ¨å•Ÿå‹•ç€è¦½å™¨ (headless={headless})...")
            browser: Browser = await p.chromium.launch(
                headless=headless,
                slow_mo=slow_mo
            )

            try:
                # å»ºç«‹æ–°é é¢
                page: Page = await browser.new_page()

                # æ­¥é©Ÿ 1: ç™»å…¥
                login_success = await self.login(page)
                if not login_success:
                    logger.info("\nâŒ ç™»å…¥å¤±æ•—ï¼Œç„¡æ³•ç¹¼çºŒ")
                    return False

                # æ­¥é©Ÿ 2: æª¢æŸ¥ä¸¦å€Ÿé–±æ›¸ç±
                borrow_result = await self.check_and_borrow_book(page, self.book_id)

                if not borrow_result:
                    logger.info("\nâŒ å€Ÿé–±å¤±æ•—")
                    return False

                # æ­¥é©Ÿ 3: å¦‚æœå•Ÿç”¨çˆ¬èŸ²ä¸”æˆåŠŸå€Ÿé–±ï¼Œé–‹å§‹çˆ¬å–å…§å®¹
                if self.enable_scraping and isinstance(borrow_result, Page):
                    reading_page = borrow_result

                    # æ ¹æ“šæ¨¡å¼é¸æ“‡ä¸åŒçš„çˆ¬å–æ–¹æ³•
                    if self.image_only_mode:
                        # ç´”åœ–ç‰‡æ›¸ç±æ¨¡å¼ï¼ˆCanvas Onlyï¼‰
                        markdown_content = await self.scrape_image_only_book(reading_page)
                    else:
                        # æ¨™æº– HTML + Canvas çˆ¬å–æ¨¡å¼
                        markdown_content = await self.scrape_entire_book(reading_page)

                    # å„²å­˜ç‚ºæª”æ¡ˆ
                    output_dir = Path("downloads")
                    output_dir.mkdir(exist_ok=True)

                    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

                    # ä½¿ç”¨æ›¸åä½œç‚ºæª”æ¡ˆåï¼ˆå¦‚æœæœ‰çš„è©±ï¼‰
                    if self.book_title:
                        # ç§»é™¤æª”æ¡ˆåä¸­ä¸å…è¨±çš„å­—å…ƒ
                        safe_title = re.sub(r'[<>:"/\\|?*]', '_', self.book_title)
                        output_file = output_dir / f"{safe_title}_{timestamp}.md"
                    else:
                        output_file = output_dir / f"book_{self.book_id}_{timestamp}.md"

                    # ç”Ÿæˆ Markdown æ¨™é¡Œ
                    # header = f"# {self.book_title if self.book_title else 'æ›¸ç±å…§å®¹'}\n\n"
                    # if self.book_title:
                    #     header += f"- æ›¸å: {self.book_title}\n"
                    # header += f"- æ›¸ç± ID: {self.book_id}\n"
                    # header += f"- çˆ¬å–æ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
                    # header += "---\n\n"

                    with open(output_file, 'w', encoding='utf-8') as f:
                        f.write(markdown_content)

                    logger.info(f"\nğŸ’¾ å·²å„²å­˜è‡³: {output_file}")
                    logger.info(f"ğŸ“Š æª”æ¡ˆå¤§å°: {output_file.stat().st_size / 1024:.2f} KB")

                    # ç­‰å¾…ä¸€æ®µæ™‚é–“è®“ä½¿ç”¨è€…çœ‹åˆ°çµæœ
                    if not headless:
                        logger.info(f"\nâ³ å°‡åœ¨ {wait_time} ç§’å¾Œé—œé–‰ç€è¦½å™¨...")
                        await asyncio.sleep(wait_time)

                    return True

                elif not self.enable_scraping:
                    # åªå€Ÿé–±ï¼Œä¸çˆ¬èŸ²
                    if not headless:
                        logger.info(f"\nâ³ å°‡åœ¨ {wait_time} ç§’å¾Œé—œé–‰ç€è¦½å™¨...")
                        await asyncio.sleep(wait_time)
                    return True

                return False

            except Exception as e:
                logger.info(f"\nâŒ åŸ·è¡Œéç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
                import traceback
                traceback.print_exc()
                return False

            finally:
                # é—œé–‰ç€è¦½å™¨
                await browser.close()
                logger.info("\nğŸ”š ç€è¦½å™¨å·²é—œé–‰")


async def main():
    """ä¸»ç¨‹å¼"""
    logger.info("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘     æ¡ƒåœ’å¸‚ç«‹åœ–æ›¸é¤¨ HyRead é›»å­æ›¸è‡ªå‹•å€Ÿé–±å·¥å…·                â•‘
â•‘                                                              â•‘
â•‘  ä½¿ç”¨ Playwright + Google Gemini API è‡ªå‹•è¾¨è­˜é©—è­‰ç¢¼         â•‘
â•‘  è‡ªå‹•ç™»å…¥ â†’ æª¢æŸ¥å¯å€Ÿæ•¸é‡ â†’ å€Ÿé–±é›»å­æ›¸                      â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)

    try:
        # åˆå§‹åŒ–å€Ÿé–±å™¨
        scraper = HyReadScraper(env_file=".env_hyread")

        # åŸ·è¡Œå€Ÿé–±æµç¨‹
        # headless=False: é¡¯ç¤ºç€è¦½å™¨è¦–çª—ï¼ˆæ–¹ä¾¿è§€å¯Ÿï¼‰
        # headless=True: ç„¡é ­æ¨¡å¼ï¼ˆé©åˆè‡ªå‹•åŒ–åŸ·è¡Œï¼‰
        # wait_time: æˆåŠŸå¾Œç­‰å¾…æ™‚é–“ï¼ˆç§’ï¼‰
        success = await scraper.run(
            headless=False,
            slow_mo=100,
            wait_time=5
        )

        if success:
            logger.info("\nâœ¨ å€Ÿé–±æµç¨‹å®Œæˆï¼")
            sys.exit(0)
        else:
            logger.info("\nâš ï¸  å€Ÿé–±æµç¨‹æœªæˆåŠŸå®Œæˆ")
            sys.exit(1)

    except FileNotFoundError as e:
        logger.info(f"\nâŒ éŒ¯èª¤: {e}")
        logger.info("\nè«‹ç¢ºä¿ä»¥ä¸‹æª”æ¡ˆå­˜åœ¨ä¸¦åŒ…å«å¿…è¦çš„è¨­å®š:")
        logger.info("   .env_hyread")
        sys.exit(1)

    except ImportError as e:
        logger.info(f"\nâŒ å¥—ä»¶éŒ¯èª¤: {e}")
        sys.exit(1)

    except ValueError as e:
        logger.info(f"\nâŒ è¨­å®šéŒ¯èª¤: {e}")
        sys.exit(1)

    except Exception as e:
        logger.info(f"\nâŒ ç™¼ç”Ÿæœªé æœŸçš„éŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    # åŸ·è¡Œä¸»ç¨‹å¼
    asyncio.run(main())

